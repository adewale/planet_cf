"""
Boris Tane feed entries fixture.
Generated from https://boristane.com/rss.xml
"""

BORISTANE_FEED_ENTRIES = [
    {
        "title": "Logging Sucks",
        "link": "https://boristane.com/blog/logging-sucks/",
        "guid": "https://boristane.com/blog/logging-sucks/",
        "description": "Why traditional logging fails in distributed systems and how wide events transform debugging from archaeology into analytics",
        "pubDate": "Sun, 21 Dec 2025 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "External link to [loggingsucks.com](https://loggingsucks.com)",
    },
    {
        "title": "What even are Cloudflare Durable Objects?",
        "link": "https://boristane.com/blog/what-are-cloudflare-durable-objects/",
        "guid": "https://boristane.com/blog/what-are-cloudflare-durable-objects/",
        "description": "Notes from 2 years of building with Cloudflare Durable Objects",
        "pubDate": "Tue, 04 Nov 2025 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": 'Two years ago I was just like you. I had heard of [Cloudflare Durable Objects](https://workers.cloudflare.com/product/durable-objects), I read the docs, but still had no idea what these were. Today I almost exclusively build with Durable Objects (DOs), and I can\'t imagine building production applications without them. Let me explain what finally made it click for me.\n\n## The Problem: State in Serverless\n\nYou know how regular serverless functions (like [AWS Lambda Functions](https://aws.amazon.com/lambda/)) are stateless? Every request starts fresh, with no memory of what happened before. If you want to remember something between requests, you have to store it in a database.\n\nThis works fine for most CRUD operations.\n\nBut it quickly becomes a real challenge when you need to:\n\n- Handle WebSocket connections that stay open for minutes or hours\n- Coordinate multiple requests to the same resource (like when 10 people are editing the same document simultaneously)\n- Run scheduled tasks for a specific user (like sending a subscription renewal reminder exactly 30 days after signup)\n\nThe traditional solution? Spin up a stateful server. But now you\'re back to managing servers, dealing with scaling, load balancing, and paying for idle capacity.\n\n```mermaid\ngraph TB\n    User[User Requests for Document 123]\n\n    User --> LB{API Gateway}\n\n    LB --> L1[Lambda A]\n    LB --> L2[Lambda B]\n    LB --> L3[Lambda C]\n\n    L1 --> Server[Central Server]\n    L2 --> Server\n    L3 --> Server\n\n    L1 -.->|❌ No coordination| L2\n    L2 -.->|❌ No coordination| L3\n    L3 -.->|❌ No coordination| L1\n\n    Server --> DB[(Central Database)]\n\n    style L1 fill:#f96,stroke:#333\n    style L2 fill:#f96,stroke:#333\n    style L3 fill:#f96,stroke:#333\n    style LB fill:#fc9,stroke:#333\n    style DB fill:#99f,stroke:#333\n    style Server fill:#9c9,stroke:#333\n```\n\nDurable Objects give you the benefits of stateful servers (long-lived connections, state, coordination) without the operational overhead. They\'re serverless, but stateful. They scale automatically, but maintain strong consistency. They\'re ephemeral (shutting down when idle), but durable (state persists).\n\n```mermaid\ngraph TB\n    User1[User 1<br/>Document 123] --> Worker1[Worker A]\n    User2[User 2<br/>Document 123] --> Worker2[Worker B]\n    User3[User 3<br/>Document 123] --> Worker3[Worker C]\n\n    Worker1 --> DO[Durable Object]\n    Worker2 --> DO\n    Worker3 --> DO\n\n    style Worker1 fill:#FAAD3F,stroke:#333\n    style Worker2 fill:#FAAD3F,stroke:#333\n    style Worker3 fill:#FAAD3F,stroke:#333\n    style DO fill:#9f9,stroke:#333,stroke-width:2px\n```\n\n## My "Aha!" Moment: One Instance per ID\n\nHere\'s the key insight that made it click for me:\n\n> A Durable Object is like having a tiny, long-lived server that is **guaranteed to be unique** for a specific ID.\n\nWhen you create a Durable Object with [`idFromName("workspace-123")`](https://developers.cloudflare.com/durable-objects/api/namespace/#idfromname), Cloudflare ensures that **only one instance** of that object exists globally. Every request with that same ID goes to the **exact same instance**, no matter where in the world the request comes from.\n\nThis is fundamentally different from regular [Cloudflare Workers](https://workers.cloudflare.com/product/workers). With Workers, if 100 requests come in for "workspace-123", they might be handled by 100 different Worker instances across 100 different data centers, depending on the origating request. Each instance is isolated, stateless, and ephemeral.\n\nWith Durable Objects, all 100 requests for "workspace-123" are routed to the **same single instance** of your Durable Object. That instance can be running in Tokyo, and requests from New York will be routed there. This single instance can maintain WebSocket connections, coordinate concurrent access, and act as the single source of truth for that workspace\'s state.\n\nIt\'s like having a dedicated micro-server for each user, workspace, or chat room. Except you only pay when it\'s actually being used, and you never have to think about deployment, scaling, or geographic distribution.\n\n## Explain Like I\'m 5\n\nImagine a library (Cloudflare\'s edge network) with millions of private study rooms (Durable Objects). Each room has:\n- A **unique room number** (the ID)\n- A **desk with drawers** (persistent storage)\n- A **person working inside** (your code)\n- A **whiteboard** (in-memory state)\n\nWhen you want to access "Room 123", the librarian always sends you to the same room. The person inside remembers what they were doing, has access to their drawers (storage), and can handle your request immediately.\n\nIf nobody visits the room for a while, the person leaves (hibernation), but their drawers stay locked. When someone comes to visit the same room again, the person returns exactly where they left off.\n\n## How Requests Flow\n\n```mermaid\ngraph TB\n    Client[Client Request] --> Worker[Cloudflare Worker]\n    Worker --> |"idFromName(\'user-123\')"| DO_ID[Durable Object ID]\n    DO_ID --> |Get Stub| Stub[Durable Object Stub]\n    Stub --> |RPC Call| Instance[Durable Object Instance]\n\n    Instance --> Memory[In-Memory State]\n    Instance --> Storage[KV Storage]\n    Instance --> SQL[SQLite Database]\n    Instance --> Alarm[Scheduled Alarms]\n\n    style Instance fill:#f9a,stroke:#333,stroke-width:4px\n    style Memory fill:#9cf,stroke:#333,stroke-width:2px\n    style Storage fill:#9f9,stroke:#333,stroke-width:2px\n    style SQL fill:#ff9,stroke:#333,stroke-width:2px\n    style Alarm fill:#f99,stroke:#333,stroke-width:2px\n```\n\n## Creating a Durable Object\n\nLet\'s build a simple "Workspace" Durable Object, based on patterns from real production code:\n\n```typescript\n// workspace.ts - The Durable Object class\nexport class DurableWorkspace extends DurableObject {\n  private sql: SqlStorage;\n\n  constructor(ctx: DurableObjectState, env: Env) {\n    super(ctx, env);\n    this.sql = ctx.storage.sql;\n\n    // Run migrations on first access\n    ctx.blockConcurrencyWhile(async () => {\n      this.migrate();\n    });\n  }\n\n  migrate() {\n    // Create tables if they don\'t exist\n    this.sql.exec(`\n      CREATE TABLE IF NOT EXISTS projects (\n        id TEXT PRIMARY KEY,\n        name TEXT NOT NULL,\n        created INTEGER NOT NULL\n      )\n    `);\n  }\n\n  // Store workspace identity\n  async init(workspaceId: string, ownerId: string) {\n    await this.ctx.storage.put("workspaceId", workspaceId);\n    await this.ctx.storage.put("ownerId", ownerId);\n\n    // Schedule a monthly cleanup\n    await this.ctx.storage.setAlarm(Date.now() + 30 * 24 * 60 * 60 * 1000);\n  }\n\n  async getWorkspaceId(): Promise<string> {\n    return await this.ctx.storage.get("workspaceId") || "";\n  }\n\n  async createProject(name: string) {\n    const id = crypto.randomUUID();\n    this.sql.exec(\n      "INSERT INTO projects (id, name, created) VALUES (?, ?, ?)",\n      id,\n      name,\n      Date.now()\n    );\n    return { id, name };\n  }\n\n  async listProjects() {\n    const result = this.sql.exec("SELECT * FROM projects ORDER BY created DESC");\n    return result.toArray();\n  }\n\n  // Called when alarm triggers\n  async alarm() {\n    console.log("Running monthly cleanup...");\n    // Clean up old data\n    this.sql.exec("DELETE FROM projects WHERE created < ?", Date.now() - 90 * 24 * 60 * 60 * 1000);\n\n    // Schedule next cleanup\n    await this.ctx.storage.setAlarm(Date.now() + 30 * 24 * 60 * 60 * 1000);\n  }\n}\n```\n\n## Accessing the Durable Object: The Stub Pattern\n\nNow that we\'ve seen a Durable Object class, how do we actually use it? You don\'t instantiate Durable Objects directly. Instead, you get a "stub" - a proxy that lets you call methods on the Durable Object from your Worker.\n\nThe critical part is the `idFromName()` call. This is what ensures you always get the same instance for the same ID. Think of it like a phone book - you look up "workspace-123" in the directory, and Cloudflare gives you a direct line to that workspace\'s unique Durable Object instance, wherever it happens to be running in the world.\n\nHere\'s how you access your Durable Object from a Worker:\n\n```typescript\n// Helper function to get a workspace stub\nexport function getWorkspaceStub(env: Env, workspaceId: string): DurableObjectStub<DurableWorkspace> {\n  // idFromName ensures the same ID always goes to the same instance\n  const id = env.WORKSPACE.idFromName(workspaceId);\n  const stub = env.WORKSPACE.get(id);\n  return stub;\n}\n\n// In your Worker (API endpoint)\nexport default {\n  async fetch(request: Request, env: Env) {\n    const url = new URL(request.url);\n    const workspaceId = url.searchParams.get("workspace");\n\n    // Get the stub - always the same instance for this workspace\n    const workspace = getWorkspaceStub(env, workspaceId);\n\n    if (url.pathname === "/init") {\n      await workspace.init(workspaceId, "user-123");\n      return new Response("Initialized");\n    }\n\n    if (url.pathname === "/projects") {\n      if (request.method === "POST") {\n        const { name } = await request.json();\n        const project = await workspace.createProject(name);\n        return Response.json(project);\n      } else {\n        const projects = await workspace.listProjects();\n        return Response.json(projects);\n      }\n    }\n\n    return new Response("Not found", { status: 404 });\n  }\n};\n```\n\nThe Worker acts as a router - it extracts the workspace ID from the request, gets the appropriate Durable Object stub using `idFromName()`, and calls methods on it.\n\nThe `workspace.createProject()` and `workspace.listProjects()` calls are [RPC (Remote Procedure Calls)](https://developers.cloudflare.com/workers/runtime-apis/rpc/) - they\'re executed inside the Durable Object instance, not in the Worker. The Worker just forwards the request and gets the response back.\n\n## The Four Storage Layers\n\nOne of the most powerful aspects of Durable Objects is having multiple storage options. When I first started, I didn\'t understand why you\'d need so many ways to store data. But each layer serves a distinct purpose and has different performance characteristics.\n\nThink of it like this: in-memory state is your desk (fast, temporary), KV storage is your desk drawer (fast, persistent, small items), SQLite is your filing cabinet (structured, queryable, medium size), and external storage like R2 is your warehouse (massive, slower, for big things).\n\nUnderstanding when to use each storage layer is crucial:\n\n```mermaid\ngraph TD\n    DO[Durable Object Instance]\n\n    DO --> KV[Key-Value Storage<br/>ctx.storage.put/get]\n    DO --> SQL[SQLite Database<br/>ctx.storage.sql]\n    DO --> Memory[In-Memory State<br/>class properties]\n    DO --> External[External Storage<br/>R2, KV Namespace, D1]\n\n    KV --> |Use For| KV_Use[Simple metadata: User ID, config, etc.]\n    SQL --> |Use For| SQL_Use[Structured data: Relations & indexes]\n    Memory --> |Use For| Memory_Use[WebSocket connections, active sessions, etc.]\n    External --> |Use For| External_Use[Large files, cross-workspace data, analytics]\n\n    style KV fill:#9f9,stroke:#333\n    style SQL fill:#ff9,stroke:#333\n    style Memory fill:#9cf,stroke:#333\n    style External fill:#f9f,stroke:#333\n```\n\n### Example: Using All Storage Layers\n\n```typescript\nexport class ChatRoom extends DurableObject {\n  // In-memory: active WebSocket connections\n  private connections: Map<string, WebSocket> = new Map();\n\n  // In-memory: temporary message buffer\n  private messageBuffer: Message[] = [];\n\n  constructor(ctx: DurableObjectState, env: Env) {\n    super(ctx, env);\n\n    ctx.blockConcurrencyWhile(async () => {\n       // SQLite: create messages table\n      this.ctx.storage.sql.exec(`\n        CREATE TABLE IF NOT EXISTS messages (\n          id TEXT PRIMARY KEY,\n          userId TEXT NOT NULL,\n          content TEXT NOT NULL,\n          timestamp INTEGER NOT NULL\n        );\n        CREATE INDEX IF NOT EXISTS idx_timestamp ON messages(timestamp);\n      `);\n    });\n  }\n\n  async initRoom(roomId: string, roomName: string) {\n    // Key-Value: store simple metadata\n    await this.ctx.storage.put("roomId", roomId);\n    await this.ctx.storage.put("roomName", roomName);\n    await this.ctx.storage.put("createdAt", Date.now());\n  }\n\n  async handleWebSocket(request: Request): Promise<Response> {\n    const pair = new WebSocketPair();\n    const [client, server] = Object.values(pair);\n\n    // In-memory: track active connection\n    const connectionId = crypto.randomUUID();\n    this.connections.set(connectionId, server);\n\n    server.addEventListener("message", async (event) => {\n      const message = JSON.parse(event.data);\n      // In-memory: save message to the buffer\n      this.messageBuffer.push(message)\n\n      // SQLite: persist message\n      this.ctx.storage.sql.exec(\n        "INSERT INTO messages (id, userId, content, timestamp) VALUES (?, ?, ?, ?)",\n        crypto.randomUUID(),\n        message.userId,\n        message.content,\n        Date.now()\n      );\n\n      // In-memory: broadcast to all active connections\n      for (const [id, conn] of this.connections) {\n        if (id !== connectionId) {\n          conn.send(JSON.stringify(message));\n        }\n      }\n\n      // External: save to R2 for archival\n      if (this.messageBuffer.length >= 100) {\n        await this.archiveMessages();\n      }\n    });\n\n    server.addEventListener("close", () => {\n      this.connections.delete(connectionId);\n    });\n\n    server.accept();\n    return new Response(null, { status: 101, webSocket: client });\n  }\n\n  async archiveMessages() {\n    const roomId = await this.ctx.storage.get("roomId");\n    const messages = this.messageBuffer.splice(0);\n\n    // External storage: R2 bucket for large archives\n    await this.env.ARCHIVE_BUCKET.put(\n      `rooms/${roomId}/${Date.now()}.json`,\n      JSON.stringify(messages)\n    );\n  }\n\n  async getRecentMessages(limit: number = 50) {\n    // SQLite: efficient queries with indexes\n    const result = this.ctx.storage.sql.exec(\n      "SELECT * FROM messages ORDER BY timestamp DESC LIMIT ?",\n      limit\n    );\n    return result.toArray();\n  }\n}\n```\n\n## Real-World Pattern: Parent-Child Relationships\n\nThis is my favourite pattern when building multitenant applications. Let me explain it with a real-world analogy first, then show you the code.\n\n### The Apartment Building Analogy\n\nImagine you\'re building a project management SaaS app. Think of it like an apartment building:\n\n- **The Building (Workspace)** is a Durable Object that represents the entire workspace\n- **Each Apartment (Project)** is its own separate Durable Object\n- **The Building Manager** keeps a directory of all apartments (the workspace\'s SQLite database)\n- **Each Apartment** has its own furniture, residents, and state (the project\'s data and tasks)\n\nWhy separate them? Because if you put all apartments\' furniture in the building manager\'s office, it gets crowded fast. Instead, each apartment manages its own internal state, but the building manager knows about all apartments and can coordinate between them.\n\nIn code terms: **don\'t put all your data in one Durable Object**. Create separate child Durable Objects for each logical entity (user, project, document, chat room), and have a parent Durable Object that coordinates them.\n\n### Why This Pattern Matters\n\nThe thing is, each Durable Object is single-threaded. If you have one giant Durable Object handling everything for a workspace with 100 projects, and 50 users are all editing different projects simultaneously, they\'re all waiting in line for that one Durable Object to process their requests.\n\nBut if each project is its own Durable Object, those 50 users can work in parallel across 50 different Durable Objects. The parent workspace just maintains a registry of which projects exist.\n\n```typescript\n// Parent: DurableWorkspace\nexport class DurableWorkspace extends DurableObject {\n  async createThread(threadName: string) {\n    const threadId = crypto.randomUUID();\n    const workspaceId = await this.ctx.storage.get("workspaceId");\n\n    // Store reference in parent\'s database\n    this.ctx.storage.sql.exec(\n      "INSERT INTO threads (id, name, created) VALUES (?, ?, ?)",\n      threadId,\n      threadName,\n      Date.now()\n    );\n\n    // Get stub to child Durable Object\n    const threadStub = getThreadStub(this.env, threadId);\n\n    // Initialize the child\n    await threadStub.init(threadId, workspaceId, threadName);\n\n    return { id: threadId, name: threadName };\n  }\n\n  async listThreads() {\n    const result = this.ctx.storage.sql.exec(\n      "SELECT * FROM threads ORDER BY created DESC"\n    );\n    return result.toArray();\n  }\n}\n\n// Child: DurableThread\nexport class DurableThread extends DurableObject {\n  async init(threadId: string, workspaceId: string, threadName: string) {\n    // Store parent reference\n    await this.ctx.storage.put("threadId", threadId);\n    await this.ctx.storage.put("workspaceId", workspaceId);\n    await this.ctx.storage.put("threadName", threadName);\n  }\n\n  async addMessage(content: string) {\n    const id = crypto.randomUUID();\n    this.ctx.storage.sql.exec(\n      "INSERT INTO messages (id, content, created) VALUES (?, ?, ?)",\n      id,\n      content,\n      Date.now()\n    );\n    return { id, content };\n  }\n\n  async deleteThread() {\n    const workspaceId = await this.ctx.storage.get("workspaceId");\n    const threadId = await this.ctx.storage.get("threadId");\n\n    // Tell parent to remove reference\n    const workspaceStub = getWorkspaceStub(this.env, workspaceId);\n    await workspaceStub.deleteThreadReference(threadId);\n\n    // Clean up own data\n    this.ctx.storage.sql.exec("DELETE FROM messages");\n    await this.ctx.storage.deleteAll();\n  }\n}\n\n// Helper functions\nfunction getThreadStub(env: Env, threadId: string) {\n  const id = env.THREAD.idFromName(threadId);\n  return env.THREAD.get(id);\n}\n\nfunction getWorkspaceStub(env: Env, workspaceId: string) {\n  const id = env.WORKSPACE.idFromName(workspaceId);\n  return env.WORKSPACE.get(id);\n}\n```\n\n### What\'s Happening Here?\n\nLet\'s walk through the flow when a user creates a thread:\n\n1. **User calls API**: "Create a thread named \'Q4 Planning\' in workspace-123"\n2. **Worker gets workspace stub**: Uses `idFromName("workspace-123")` to get the workspace Durable Object\n3. **Workspace creates reference**: Stores "thread-456, Q4 Planning" in its own SQLite database\n4. **Workspace spawns child**: Gets a stub to a new Thread Durable Object using `idFromName("thread-456")`\n5. **Child initializes**: The thread stores its identity (threadId, workspaceId, name)\n6. **Now they\'re connected**: The workspace knows about the thread, and the thread knows about its parent workspace\n\nLater, when listing threads:\n- You ask the workspace: "What threads exist?"\n- It queries its own database and returns the list\n- You don\'t need to wake up every thread Durable Object just to list them\n\nWhen adding a message:\n- You go directly to the thread Durable Object\n- It handles the message independently\n- The workspace doesn\'t need to be involved at all\n\nThis separation means the workspace can track hundreds of threads without getting bogged down by message operations, and threads can process messages in parallel without waiting for the workspace.\n\n## Scheduled Operations with Alarms\n\nHere\'s another outstanding feature that took me a while to appreciate: each Durable Object can schedule its own future work. Not through some external scheduler, but built right into the object itself.\n\nThink about subscription renewals. With traditional serverless, you\'d need:\n1. A cron job that runs every hour\n2. It queries your database for all subscriptions expiring in the next hour\n3. It processes each one\n4. This gets expensive and slow as you scale to millions of users\n\nWith Durable Objects + Alarms:\n1. When a user subscribes, their Subscription Durable Object schedules an alarm for exactly 30 days from now\n2. In 30 days, that specific Durable Object wakes up automatically\n3. It processes just that user\'s renewal\n4. Then schedules the next alarm\n\nNo cron jobs scanning millions of records. No batch processing. Just-in-time execution for each user. This is incredibly powerful for per-user or per-entity scheduled tasks.\n\nHere\'s how it works in code:\n\n```typescript\nexport class SubscriptionManager extends DurableObject {\n  async createSubscription(userId: string, planType: string) {\n    await this.ctx.storage.put("userId", userId);\n    await this.ctx.storage.put("planType", planType);\n    await this.ctx.storage.put("status", "active");\n\n    // Schedule renewal check in 30 days\n    const renewalDate = Date.now() + 30 * 24 * 60 * 60 * 1000;\n    await this.ctx.storage.setAlarm(renewalDate);\n  }\n\n  async alarm() {\n    const userId = await this.ctx.storage.get("userId");\n    const planType = await this.ctx.storage.get("planType");\n\n    console.log(`Processing renewal for user ${userId}`);\n\n    // Charge the user\n    const success = await this.chargeUser(userId, planType);\n\n    if (success) {\n      // Schedule next renewal\n      await this.ctx.storage.setAlarm(Date.now() + 30 * 24 * 60 * 60 * 1000);\n    } else {\n      // Downgrade to free plan\n      await this.ctx.storage.put("status", "expired");\n      await this.ctx.storage.put("planType", "free");\n    }\n  }\n\n  async cancelSubscription() {\n    await this.ctx.storage.put("status", "cancelled");\n\n    // Cancel future alarms\n    await this.ctx.storage.deleteAlarm();\n  }\n\n  private async chargeUser(userId: string, planType: string): Promise<boolean> {\n    // Call payment API\n    return true;\n  }\n}\n```\n\nWhen you schedule an alarm, Cloudflare guarantees it will fire at least once. If your `alarm()` method throws an exception, it automatically retries up to 6 times with exponential backoff (starting at 2 seconds).\n\nYou don\'t need to write retry logic, just make sure your `alarm()` method is idempotent (safe to run multiple times) since it could potentially execute more than once.\n\n## When Should You Use Durable Objects?\n\nHere\'s my mini-decision framework:\n\n**Perfect for Durable Objects:**\n- Per-user/per-tenant databases\n- AI agents\n- Real-time collaboration (Google Docs-style editing)\n- WebSocket servers (chat rooms, live dashboards)\n- Coordinating multiple requests to the same resource\n- Rate limiting per user\n- Session management\n- Scheduled tasks per entity (user notifications, subscription renewals)\n\n**Not ideal for Durable Objects:**\n- Storing large blobs (use [R2](https://workers.cloudflare.com/product/r2) instead)\n- Analytics aggregation across many users ([Workers Analytics Engine](https://developers.cloudflare.com/analytics/analytics-engine/))\n- Truly stateless operations (use regular [Workers](https://workers.cloudflare.com/product/workers))\n- Queries across all users (use [D1](https://workers.cloudflare.com/product/d1) with proper indexes)\n\n**Be careful with:**\n- Very high traffic to a single ID (DOs are single-threaded per instance)\n- Long-running CPU-intensive tasks (use queues + regular workers instead)\n\n## Common Pitfalls I Learned the Hard Way\n\nLet me save you from the mistakes I made when I was learning Durable Objects. These gotchas aren\'t obvious from the docs, but they\'ll bite you in production if you\'re not careful.\n\n### 1. Don\'t Forget: One Instance = One Thread\n\nThis is the biggest gotcha. A Durable Object instance is single-threaded. Every request to the same ID is processed sequentially, one at a time. This is actually a feature (strong consistency!), but it means you can\'t block for long periods.\n\nI once had a Durable Object that called a slow external API and took 5 seconds to respond. When 10 requests came in simultaneously for the same object, the last request waited 50 seconds. Not good.\n\n```typescript\n// BAD: Blocking operation will freeze this DO for everyone\nexport class BadExample extends DurableObject {\n  async handleRequest() {\n    // This blocks for 10 seconds - ALL requests wait!\n    await new Promise(resolve => setTimeout(resolve, 10000));\n    return "Done";\n  }\n}\n\n// GOOD: Offload heavy work to queues\nexport class GoodExample extends DurableObject {\n  async handleRequest() {\n    // Queue the work, return immediately\n    await this.env.QUEUE.send({ task: "heavy-work" });\n    return "Queued";\n  }\n}\n```\n\n### 2. Initialize the Durable Object identity\n\nWhen a Durable Object is first created, its storage is empty. It\'s not possible to recall the name of the Durable Object from within the Durable Object. You need to explicitly initialize it. I use an `init()` method.\n\n```typescript\n// BAD: Storage might be empty on first call\nexport class BadExample extends DurableObject {\n  async getWorkspaceId() {\n    return await this.ctx.storage.get("workspaceId"); // Could be undefined!\n  }\n}\n\n// GOOD: Explicit initialization\nexport class GoodExample extends DurableObject {\n  async init(workspaceId: string) {\n    await this.ctx.storage.put("workspaceId", workspaceId);\n  }\n\n  async getWorkspaceId() {\n    const id = await this.ctx.storage.get("workspaceId");\n    if (!id) throw new Error("Workspace not initialized - call init() first");\n    return id;\n  }\n}\n```\n\n## The Key to Real-Time Apps and AI Agents: Hibernate API\n\nBefore [Hibernate API](https://developers.cloudflare.com/workers/platform/changelog/#2023-05-26), if you had a chat room with 1000 connected users but nobody was sending messages, you were still paying for that Durable Object to stay alive and hold those connections.\n\nWith Hibernate API, the Durable Object can "hibernate" - essentially go to sleep while maintaining WebSocket connections. When a message arrives, it wakes up automatically, handles the message, and can go back to sleep.\n\nThe beauty is that this happens transparently. Your code doesn\'t change. You just get automatic cost savings and better resource utilization.\n\n```typescript\nexport class ChatRoom extends DurableObject {\n  // Hibernate API automatically serializes/deserializes state\n  async webSocketMessage(ws: WebSocket, message: string) {\n    const data = JSON.parse(message);\n\n    // Broadcast to all connections\n    this.ctx.getWebSockets().forEach(socket => {\n      socket.send(JSON.stringify(data));\n    });\n  }\n\n  async webSocketClose(ws: WebSocket, code: number, reason: string) {\n    console.log("disconnected");\n  }\n\n  async handleConnect(request: Request): Promise<Response> {\n    const pair = new WebSocketPair();\n    const [client, server] = Object.values(pair);\n\n    // Attach metadata to the WebSocket\n    this.ctx.acceptWebSocket(server);\n\n    return new Response(null, { status: 101, webSocket: client });\n  }\n}\n```\n\nWith Hibernate API, your Durable Object can go to sleep when idle (saving you money) and wake up automatically when a WebSocket message arrives. The state is automatically preserved!\n\n## Leveling Up: The Cloudflare Agents SDK\n\nThe Cloudflare [`agents` SDK](https://agents.cloudflare.com/) ([`npm i agents`](https://www.npmjs.com/package/agents)) is a framework built on top of Durable Objects. It\'s designed for building AI agents on the Cloudflare network, and has become my default framework when building with Durable Objects.\n\n**You don\'t need to be building AI agents to use it.** Despite the name, it\'s actually a general-purpose framework for building stateful real-time applications with Durable Objects. It just happens to make AI agent development really easy too.\n\n### What Does the Agents SDK Give You?\n\nThe `Agent` base class provides:\n\n1. **WebSocket handling out of the box** - `onConnect()`, `onMessage()`, `onClose()` lifecycle methods\n2. **HTTP request handling** - `onRequest()` method with clean routing\n3. **Email routing** - Built-in email address parsing and routing (yes, Durable Objects can receive emails!)\n4. **Scheduled operations** - Simplified alarm scheduling with cron expressions\n5. **Task queues** - Lightweight queue for task defferal\n6. **Hibernation API** - Automatic connection state management for WebSockets\n\n### A Simple Example: Chat Room Without AI\n\nHere\'s a chat room using the Agents SDK - notice there\'s no AI involved, it\'s just using the framework\'s convenience features:\n\n```typescript\n\nexport class ChatRoom extends Agent {\n  // WebSocket lifecycle - automatically handled\n  async onConnect(connection, ctx) {\n    const username = ctx.request.headers.get("username");\n\n    // Set connection metadata (persists through hibernation!)\n    await connection.setState({ username, joinedAt: Date.now() });\n\n    // Broadcast join message to everyone\n    this.broadcast(JSON.stringify({\n      type: "user_joined",\n      username,\n      timestamp: Date.now()\n    }));\n  }\n\n  override async onMessage(connection, message) {\n    const data = JSON.parse(message);\n    const state = await connection.getState();\n\n    // Save to SQLite for history\n    this.ctx.storage.sql.exec(\n      "INSERT INTO messages (id, username, content, timestamp) VALUES (?, ?, ?, ?)",\n      crypto.randomUUID(),\n      state.username,\n      data.content,\n      Date.now()\n    );\n\n    // Broadcast to all connected clients (except sender)\n    this.broadcast(JSON.stringify({\n      type: "message",\n      username: state.username,\n      content: data.content,\n      timestamp: Date.now()\n    }), [connection.id]);\n  }\n\n  async onClose(connection) {\n    const state = await connection.getState();\n\n    this.broadcast(JSON.stringify({\n      type: "user_left",\n      username: state.username,\n      timestamp: Date.now()\n    }));\n  }\n\n  // Regular HTTP request handling\n  override async onRequest(request) {\n    // Get recent message history via HTTP\n    if (request.method === "GET") {\n      const messages = this.ctx.storage.sql.exec(\n        "SELECT * FROM messages ORDER BY timestamp DESC LIMIT 50"\n      );\n\n      return Response.json(messages.toArray());\n    }\n\n    return super.onRequest(request);\n  }\n}\n```\n\n### What This Abstracts Away\n\nCompare this to raw Durable Objects code:\n\n**Without Agents SDK:**\n```typescript\n// You manually create WebSocketPair\nconst pair = new WebSocketPair();\nconst [client, server] = Object.values(pair);\n\n// Manually track connections in a Map\nthis.connections.set(id, server);\n\n// Manually add event listeners\nserver.addEventListener("message", (event) => { /* ... */ });\nserver.addEventListener("close", () => { /* ... */ });\n\n// Manually accept\nserver.accept();\nreturn new Response(null, { status: 101, webSocket: client });\n```\n\n**With Agents SDK:**\n```typescript\n// Just implement the lifecycle methods\nasync onConnect(connection, ctx) { /* ... */ }\nasync onMessage(connection, message) { /* ... */ }\nasync onClose(connection) { /* ... */ }\n\n// Framework handles all the boilerplate\n```\n\n### Using Agents SDK for Non-AI Applications\n\nI use the Agents SDK for:\n\n- **Real-time dashboards** - WebSocket connections with automatic reconnection handling\n- **Multiplayer games** - Each game room is an Agent with connection management built-in\n- **Collaborative editing** - Document state + WebSocket broadcasting without manual connection tracking\n- **Job queues** - Queue message handling with automatic retries\n- **Scheduled tasks** - Cron-based processing per entity (user notifications, report generation)\n\nThe framework handles connection lifecycle, hibernation, state persistence, and broadcasting - all the tedious parts of building with Durable Objects.\n\nI use the Agents SDK for all my Durable Objects, both my production apps and my experiments. The base `Agent` class provides so much value that even for non-AI applications, it\'s worth the dependency.\n\n## Conclusion\n\nTwo years ago, I read the Durable Objects docs three times and still didn\'t get it. The breakthrough came when I stopped trying to understand the technology and started thinkering.\n\n**Durable Objects aren\'t just "stateful Workers"**. They\'re a fundamentally different primitive. They give you:\n\n- **Coordination**: One instance per ID means you can coordinate concurrent access without distributed locking\n- **Real-time**: WebSocket connections that survive across requests\n- **Consistency**: Strong consistency without the complexity of distributed systems\n- **Scheduling**: Per-entity alarms that fire exactly when needed\n- **Isolation**: Perfect tenant isolation in multi-tenant SaaS (each tenant gets its own Durable Object)\n\nThe mental model that finally clicked for me: **Durable Objects are like having millions of tiny, specialized servers that spawn on-demand, live exactly where they\'re needed, and automatically shut down when idle**.\n\nYou don\'t need to understand how Cloudflare routes requests globally to the same instance. You don\'t need to worry about replication or consistency. You just write code as if you have a single-threaded server for each user/room/workspace, and Cloudflare handles the rest.\n\n### Start Simple, Scale Up\n\nDon\'t try to use all the features at once. Start with:\n\n1. Create a Durable Object with just KV storage. Get comfortable with stubs and `idFromName`.\n2. Add SQLite for structured data. Learn about transactions.\n3. Try parent-child relationships. Understand when to split data across multiple DOs.\n4. Add WebSockets or alarms based on your use case.\n\nThe key insights I wish I\'d understood from day one:\n\n1. **One instance per ID** - This is the superpower. Embrace it.\n2. **Single-threaded** - Don\'t block. Offload heavy work to queues.\n3. **Multiple storage layers** - KV for metadata, SQLite for data, memory for connections.\n4. **Parent-child pattern** - Don\'t put everything in one DO. Split by entity.\n5. **Hibernation is magic** - Your DO can sleep while maintaining state and connections.\n\nTwo years ago, I couldn\'t wrap my head around Durable Objects. Today, they\'re the foundation of everything I build. They\'ve replaced Redis, PostgreSQL, WebSocket servers, cron jobs, and state machines in my architecture.\n\nOnce it clicks, you\'ll wonder how you ever built without them.',
    },
    {
        "title": "Unlimited On-Demand Graph Databases with Cloudflare Durable Objects",
        "link": "https://boristane.com/blog/durable-objects-graph-databases/",
        "guid": "https://boristane.com/blog/durable-objects-graph-databases/",
        "description": "How to create isolated, scalable graph databases using Cloudflare Durable Objects and SQLite for multi-tenant applications",
        "pubDate": "Mon, 27 Oct 2025 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": 'I found myself needing to create unlimited, isolated graph databases for an AI agent I\'m building. I figured an elegant way using [Durable Objects](https://developers.cloudflare.com/durable-objects/) on the [Cloudflare Developers Platform](https://workers.dev). Each graph gets its own instance with dedicated SQLite storage, enabling true multi-tenancy without the complexity of traditional database partitioning.\n\nTraditional graph database deployments require careful infrastructure planning. You need to decide upfront how many database instances to provision, configure connection pooling, implement tenant isolation at the query level, and manage scaling as your user base grows. What if you could skip all of that and let the platform handle it?\n\nIn this blog post, I\'ll walk through building a simple and extensible graph database system leveraging SQLite on Durable Objects where creating a new database is as simple as making an HTTP request with a new identifier. No provisioning, no configuration, just instant isolation.\n\n## The Multi-Tenancy Problem\n\nIf you\'re building any kind of multi-tenant application today, you\'re probably dealing with one of these patterns:\n\n1. **Shared database with tenant IDs**: Every query needs `WHERE tenant_id = ?`\n2. **Schema per tenant**: Complex migrations and connection management\n3. **Traditional database per tenant**: Infrastructure overhead and cost\n\nThe key problem is architectural complexity. When you store multiple tenants in a shared database, you\'re constantly guarding against data leakage. One missing `WHERE` clause and you\'ve exposed customer data. Likewise, when you create separate databases per tenant, you\'re managing infrastructure at scale, which usually is resource inefficient.\n\nGraph databases add another layer of complexity. Graph queries naturally traverse relationships, making it even easier to accidentally cross tenant boundaries.\n\nTraditional solutions require:\n\n1. **Careful query design**: Every traversal must check tenant ownership\n2. **Complex access control**: Node and edge-level security policies\n3. **Performance trade-offs**: Filtering by tenant ID on every hop\n\nWhat if each tenant could have their own isolated graph database, created automatically on first access?\n\n## Durable Objects as Micro-Databases\n\n[Cloudflare Durable Objects](https://developers.cloudflare.com/durable-objects/) solve this elegantly. Each Durable Object instance is a globally-unique, [strongly-consistent](https://en.wikipedia.org/wiki/Strong_consistency) compute environment with built-in SQLite storage. The key insight: **treat each Durable Object as a complete database instance**.\n\nWith Durable Objects, you get:\n\n- **Automatic isolation**: Each instance has its own SQLite database\n- **Deterministic routing**: Each `graphId` always routes to the same instance\n- **Zero provisioning**: Instances are created on-demand\n- **Global distribution**: Automatically deployed to Cloudflare\'s edge network\n\nThis enables a powerful pattern: **one graph database per identifier**.\n\n```typescript\n// This is all you need to get an isolated database\nconst doId = env.GraphDatabase.idFromName("user-123-graph");\nconst stub = env.GraphDatabase.get(doId);\n```\n\nThe first time you access `user-123-graph`, Cloudflare creates a new Durable Object instance. The second time, you get the same instance. It\'s deterministic, isolated, and automatic.\n\n## Project Overview\n\nWe\'ll be using:\n\n- [**Cloudflare Workers**](https://developers.cloudflare.com/workers/) - Serverless HTTP handling and routing\n- [**Cloudflare Durable Objects**](https://developers.cloudflare.com/durable-objects/) - Stateful instances with SQLite\n- [**Hono**](https://hono.dev/) - Lightweight web framework\n- [**TypeScript**](https://www.typescriptlang.org/) - Type-safe development\n\nHere\'s the basic structure:\n\n```\n.\n├── bindings.ts           # TypeScript bindings\n├── src\n│   ├── index.ts         # Worker router and API endpoints\n│   ├── do.ts            # Durable Object graph database implementation\n│   └── migrate.ts       # Migration system\n├── migrations\n│   ├── index.ts         # Migration registry\n│   └── v00_add_graph_tables.ts  # Initial schema\n├── web\n│   ├── index.html       # Graph visualization UI\n│   ├── app.js           # Frontend application\n│   └── style.css        # Styles\n├── wrangler.json        # Cloudflare configuration\n└── package.json         # Dependencies\n```\n\nThe architecture looks like this:\n\n```mermaid\ngraph TB\n    Client[Client/Browser] -->|HTTP Request| Worker[Cloudflare Worker - Hono Router]\n\n    Worker -->|Extract graphId from URL| Router{URL Router}\n\n    Router -->|idFromName graphId| NS[Durable Object Namespace]\n\n    NS -->|Get or Create Instance| DO[GraphDatabase Durable Object Instance]\n\n    DO -->|Raw SQL Operations| SQL[(SQLite Storage)]\n\n    SQL -->|schema_history| Migrations[Migration System]\n    SQL -->|nodes table| Nodes[Nodes: id, type, data, timestamps]\n    SQL -->|edges table| Edges[Edges: source, target, relationship]\n\n    DO -->|Graph Operations| Ops[BFS Traversal, Pathfinding, Neighbor Queries]\n\n    Ops -->|Read/Write| SQL\n\n    DO -->|JSON Response| Worker\n    Worker -->|HTTP Response| Client\n\n    style Worker fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style DO fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style SQL fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style NS fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n```\n\nThe complete source code is available on [GitHub](https://github.com/boristane/cloudflare-dev-101/tree/main/durable-objects-graph-database). I suggest following along with the code for reference.\n\n## Setting Up the Worker\n\nLet\'s start with the Worker configuration. Create a `wrangler.json` file:\n\n```json\n{\n  "name": "durable-objects-graph-database",\n  "compatibility_date": "2025-10-27",\n  "workers_dev": true,\n  "main": "./src/index.ts",\n  "assets": {\n    "directory": "./web",\n    "binding": "ASSETS"\n  },\n  "migrations": [\n    {\n      "new_sqlite_classes": ["GraphDatabase"],\n      "tag": "v1"\n    }\n  ],\n  "durable_objects": {\n    "bindings": [\n      {\n        "name": "GraphDatabase",\n        "class_name": "GraphDatabase"\n      }\n    ]\n  }\n}\n```\n\nThe `new_sqlite_classes` migration is crucial. It enables [SQLite storage](https://developers.cloudflare.com/durable-objects/api/sqlite-storage-api/) in your Durable Objects. Without this, your Durable Objects will not have a SQLite database attached.\n\nThe [`assets` configuration](https://developers.cloudflare.com/workers/static-assets/) serves a web interface directly from the Worker, enabling us to visualise our graph databases.\n\n## Building the Graph Database Durable Object\n\nThe `GraphDatabase` Durable Object implements the graph database. It\'s a class extending the `DurableObject` class. It encapsulates all graph operations and manages its own SQLite database.\n\n### Schema Design\n\n[Graph databases](https://en.wikipedia.org/wiki/Graph_database) are different from traditional relational databases. Instead of thinking in individual tables with joins, you think in **nodes** (entities) and **edges** (relationships).\n\n#### Understanding Nodes and Edges\n\nImagine modeling an organization:\n\n```mermaid\ngraph LR\n    Alice[("Alice - Engineering Manager")]\n    Bob[("Bob - Senior Engineer")]\n    Project[("Project X - Active")]\n\n    Alice -->|manages| Bob\n    Bob -->|works_on| Project\n\n    style Alice fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style Bob fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style Project fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n```\n\n**Nodes** are the "things": Alice, Bob, and Project X. Each node has:\n- An **ID** (unique identifier)\n- A **type** (person, project, document, etc.)\n- **Data** (arbitrary JSON properties like name, role, status)\n\n**Edges** are the connections: "Alice manages Bob" and "Bob works on Project X". Each edge has:\n- A **source** (where the edge starts)\n- A **target** (where the edge ends)\n- A **relationship** (what type of connection this is)\n- Optional **properties** (metadata about the relationship)\n\nThe power comes from traversing these relationships. To answer "what projects does Alice\'s team work on?", you traverse: Alice → manages → Bob → works_on → Project X.\n\n#### The Schema\n\nHere\'s how we model this in SQLite:\n\n```sql title="migrations/v00_add_graph_tables.ts"\nCREATE TABLE IF NOT EXISTS nodes (\n    id TEXT PRIMARY KEY,            -- Unique identifier (e.g., "alice", "project-x")\n    type TEXT NOT NULL,             -- Category (e.g., "person", "project")\n    data TEXT NOT NULL,             -- JSON blob: {"name": "Alice", "role": "Manager"}\n    created_by TEXT NOT NULL,       -- Who created this node\n    last_edited_by TEXT NOT NULL,   -- Who last modified it\n    updated INTEGER NOT NULL,       -- Last update timestamp (ms since epoch)\n    created INTEGER NOT NULL        -- Creation timestamp (ms since epoch)\n);\n\nCREATE TABLE IF NOT EXISTS edges (\n    source TEXT NOT NULL,           -- Source node ID\n    target TEXT NOT NULL,           -- Target node ID\n    relationship TEXT NOT NULL,     -- Relationship type (e.g., "manages", "works_on")\n    properties TEXT,                -- Optional JSON: {"since": "2024", "hours": 40}\n    created_by TEXT NOT NULL,\n    last_edited_by TEXT NOT NULL,\n    updated INTEGER NOT NULL,\n    created INTEGER NOT NULL,\n    UNIQUE(source, target, relationship)  -- Prevents duplicate edges\n);\n```\n\n#### Why This Design Works\n\n**Flexible Node Storage**: The `data` field stores JSON as text. This means different node types can have completely different properties:\n\n```json\n// Person node\n{"name": "Alice", "role": "Engineering Manager", "email": "alice@example.com"}\n\n// Project node\n{"name": "Project X", "status": "active", "deadline": "2025-12-31"}\n\n// Document node\n{"title": "Architecture Doc", "url": "https://...", "embedding": [...]}\n```\n\nYou don\'t need to alter the schema when adding new node types or properties. Just store different JSON.\n\n**Multi-Relationship Support**: The `UNIQUE(source, target, relationship)` constraint ensures the uniqueness of relationships between nodes, and enables multiple relationship types between the same two nodes:\n\n```mermaid\ngraph LR\n    A[("Alice")]\n    B[("Bob")]\n\n    A -->|manages| B\n    A -->|mentors| B\n    A -->|collaborates_with| B\n\n    style A fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style B fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n```\n\nAlice can simultaneously manage, mentor, and collaborate with Bob. Each is a separate edge with a different `relationship` value.\n\n**Directional Relationships**: Edges have direction. "Alice manages Bob" is different from "Bob manages Alice". The `source` and `target` fields encode this direction:\n\nFor bidirectional relationships (like "friends_with"), you create two edges:\n\n```mermaid\ngraph LR\n    A[("Alice")]\n    B[("Bob")]\n\n    A -->|friends_with| B\n    B -->|friends_with| A\n\n    style A fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style B fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n```\n\n**Indexes**: We add indexes to improve query performance:\n\n```sql\nCREATE INDEX edges_source_idx ON edges(source);  -- Fast outbound queries\nCREATE INDEX edges_target_idx ON edges(target);  -- Fast inbound queries\nCREATE INDEX nodes_type_idx ON nodes(type);      -- Filter by node type\n```\n\nWhen you ask "who does Alice manage?", the database uses `edges_source_idx` to quickly find all edges where `source = \'alice\'`. Without this index, it would scan a large number of edge. Query performance would tank for large graphs.\n\nSimilarly, "who manages Alice?" uses `edges_target_idx` to find edges where `target = \'alice\'`.\n\nThe `nodes_type_idx` lets you efficiently query "find all person nodes" or "find all projects" without scanning every node.\n\n### Migration System\n\nDurable Object provide a SQLite interface, but it\'s up to us to figure out a migration system. The database schema will evolve over time as our application evolves. The migration system is **idempotent** and **version-tracked**. Each Durable Object instance maintains its own migration state in a `schema_history` table.\n\nThe migration logic:\n\n1. Create `schema_history` table if it doesn\'t exist\n2. Compare current schema version with available migrations\n3. Apply any pending migrations in order within a transaction\n4. Record each migration in the history table\n\n```typescript title="src/migrate.ts"\nexport function migrate(storage: DurableObjectStorage, migrations: SQLMigration[]) {\n  createSchemaHistory(storage);\n  const current = getSchema(storage);\n  const toApply = migrations.filter((m) => m.version > (current?.version || -1));\n\n  storage.transactionSync(() => {\n    for (const migration of toApply) {\n      storage.sql.exec(migration.sql);\n      storage.sql.exec(\n        "INSERT INTO schema_history (version, name) VALUES (?, ?)",\n        migration.version, migration.name\n      );\n    }\n  });\n}\n```\n\nThe [`transactionSync`](https://developers.cloudflare.com/durable-objects/api/sqlite-storage-api/#transactionsync) wrapper ensures atomicity; either all migrations succeed or none do. This prevents corrupted schema states if a migration fails partway through.\n\nThe migrations must run in the constructor of the Durable Object class. When a Durable Object receives a request, if it\'s not yet in memory on the Cloudflare network, it is initialised and the migrations run "just-in-time" for this given instance of the Durable Object class.\n\n### Durable Object Implementation\n\nNow let\'s implement the `GraphDatabase` class:\n\n```typescript title="src/do.ts"\n\nexport class GraphDatabase extends DurableObject {\n  private db: SqlStorage;\n\n  constructor(ctx: DurableObjectState, env: Bindings) {\n    super(ctx, env);\n    this.db = ctx.storage.sql;\n\n    this.ctx.blockConcurrencyWhile(async () => {\n      migrate(ctx.storage, migrations);\n    });\n  }\n}\n```\n\nThe `blockConcurrencyWhile` ensures migrations complete before any other requests are processed. Without this, you could have race conditions where a request tries to query a table that doesn\'t exist yet.\n\n### Node Operations\n\nWe need operations to create, read, update, and delete nodes. The key node methods in the `GraphDatabase` class are:\n\n- **addNode()**: Creates or updates a node with JSON data stored as text\n- **findNode()**: Retrieves a single node by its ID\n- **findNodes()**: Lists nodes with optional filtering by type, a limit, and an offset\n- **deleteNode()**: Removes a node and cascades to delete all connected edges, ensuring the integrity of the graph\n\n```typescript title="src/do.ts"\naddNode(node: InsertNode): Node {\n  const { id, type, data, userId } = node;\n  const result = this.db.exec(\n    "INSERT OR REPLACE INTO nodes (...) VALUES (...) RETURNING *",\n    id, type, JSON.stringify(data), userId, userId, Date.now(), Date.now()\n  );\n\n  return toNode(result.toArray()[0]);\n}\n\nfindNode(id: string): Node | null {\n  const result = this.db.exec("SELECT * FROM nodes WHERE id = ?", id);\n  const res = result.toArray();\n  if (!res.length) {\n    return null;\n  }\n\n  const rows = res as Array<DBNode>;\n  const row = rows[0];\n  if (!row) return null;\n\n  return toNode(row);\n}\n\nfindNodes(filters?: { type?: string; limit?: number; offset?: number }): Node[] {\n  let query = "SELECT * FROM nodes";\n  const params: any[] = [];\n\n  if (filters?.type) {\n    query += " WHERE type = ?";\n    params.push(filters.type);\n  }\n\n  if (filters?.limit) {\n    query += " LIMIT ?";\n    params.push(filters.limit);\n  }\n\n  if (filters?.offset) {\n    query += " OFFSET ?";\n    params.push(filters.offset);\n  }\n\n  const result = this.db.exec(query, ...params);\n  const res = result.toArray();\n\n  return res.map((row) => toNode(row as DBNode));\n}\n\ndeleteNode(id: string): Node | null {\n  // First delete all edges connected to this node\n  this.db.exec("DELETE FROM edges WHERE source = ? OR target = ?", id, id);\n\n  // Then delete the node\n  const result = this.db.exec("DELETE FROM nodes WHERE id = ? RETURNING *", id);\n  const res = result.toArray();\n  if (!res.length) {\n    return null;\n  }\n\n  const rows = res as Array<DBNode>;\n  const row = rows[0];\n  if (!row) return null;\n\n  return toNode(row);\n}\n```\n\n### Edge Operations\n\n#### How Edges Work\n\nConsider this scenario: you want to model how team members contribute to projects:\n\n```mermaid\ngraph LR\n    Alice[("Alice - Manager")]\n    Bob[("Bob - Engineer")]\n    Charlie[("Charlie - Engineer")]\n    ProjectX[("Project X")]\n    ProjectY[("Project Y")]\n\n    Alice -->|manages| Bob\n    Alice -->|manages| Charlie\n    Bob -->|works_on: 40 hrs/week| ProjectX\n    Charlie -->|works_on: 20 hrs/week| ProjectX\n    Charlie -->|works_on: 20 hrs/week| ProjectY\n\n    style Alice fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style Bob fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style Charlie fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style ProjectX fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style ProjectY fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n```\n\nEach edge stores:\n\n```typescript\n// Bob works on Project X\n{\n  source: "bob",\n  target: "project-x",\n  relationship: "works_on",\n  properties: { hours_per_week: 40, role: "lead" }\n}\n\n// Charlie works on Project X\n{\n  source: "charlie",\n  target: "project-x",\n  relationship: "works_on",\n  properties: { hours_per_week: 20, role: "contributor" }\n}\n```\n\nNotice how the same relationship type (`works_on`) can have different properties. This lets you capture nuance, both Bob and Charlie work on Project X, but with different time commitments and roles.\n\n#### The Composite Key Pattern\n\nThe `UNIQUE(source, target, relationship)` constraint is fundamental to the design. It means:\n\n- Alice can both "manages" and "mentors" Bob (different relationships)\n- Alice can "manages" both Bob and Charlie (different targets)\n- Alice "manages" Bob can\'t be inserted twice (duplicate prevented)\n\n```typescript\n// First call: creates the edge\nawait connectNodes({\n  source: "bob",\n  target: "project-x",\n  relationship: "works_on",\n  properties: { hours_per_week: 30 }\n});\n\n// Second call: updates the same edge (changes hours from 30 to 40)\nawait connectNodes({\n  source: "bob",\n  target: "project-x",\n  relationship: "works_on",\n  properties: { hours_per_week: 40 }\n});\n```\n\n#### Edge Methods\n\nThe core edge operations are:\n\n**connectNodes()** - Creates or updates an edge:\n```typescript\nconst edge = await stub.connectNodes({\n  source: "alice",\n  target: "bob",\n  relationship: "manages",\n  properties: { since: "2024-01-15", team: "engineering" },\n  userId: "system"\n});\n```\n\n**disconnectNodes()** - Removes a specific edge:\n```typescript\n// Remove the "manages" relationship (other relationships remain)\nawait stub.disconnectNodes("alice", "bob", "manages");\n```\n\n**findEdges()** - Query edges by node or relationship:\n```typescript\n// Find all edges connected to Bob (any direction)\nconst bobEdges = await stub.findEdges({ nodeId: "bob" });\n\n// Find all "manages" relationships in the graph\nconst managesEdges = await stub.findEdges({ relationship: "manages" });\n```\n\n#### Edge Properties: When to Use Them\n\nSome relationships are simple facts ("knows", "contains", "references"). Others carry context that\'s essential to understanding the relationship:\n\n```typescript\n// Simple edge - no properties needed\n{ source: "doc-1", target: "doc-2", relationship: "references" }\n\n// Rich edge - properties add crucial context\n{\n  source: "alice",\n  target: "project-x",\n  relationship: "contributed_to",\n  properties: {\n    role: "architect",\n    startDate: "2024-01-01",\n    endDate: "2024-06-30",\n    linesOfCode: 15420,\n    commits: 247\n  }\n}\n```\n\nThe flexibility to add properties without schema changes lets you evolve your data model. You can start with simple edges and enrich them later as you discover what metadata matters.\n\n### Graph Traversal\n\nGraph traversal enables your to explore relationships and answer questions like "how are these two entities connected?" or "what\'s reachable from this starting point?"\n\n#### Directional Queries\n\nThe `findNeighbors()` method finds all nodes directly connected to a given node. Direction matters:\n\n```mermaid\ngraph TB\n    subgraph "Both (all Alice\'s connections)"\n    D2[("David")]\n    A3[("Alice")]\n    B2[("Bob")]\n    C2[("Charlie")]\n    D2 -->|manages| A3\n    A3 -->|manages| B2\n    A3 -->|manages| C2\n    style A3 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style D2 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style B2 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style C2 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    end\n\n    subgraph "Outbound (who does Alice manage?)"\n    A1[("Alice")]\n    B1[("Bob")]\n    C1[("Charlie")]\n    A1 -->|manages| B1\n    A1 -->|manages| C1\n    style A1 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style B1 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style C1 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    end\n\n    subgraph "Inbound (who manages Alice?)"\n    D[("David - VP Engineering")]\n    A2[("Alice")]\n    D -->|manages| A2\n    style D fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style A2 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    end\n```\n\nDirectional queries are lets you model asymmetric relationships. "Alice manages Bob" is directional, the reverse isn\'t automatically true.\n\n```typescript title="src/do.ts"\nfindNeighbors(\n  nodeId: string,\n  direction: "inbound" | "outbound" | "both" = "both"\n): string[] {\n  let query: string;\n  const params: string[] = [];\n\n  switch (direction) {\n    case "inbound":\n      query = "SELECT DISTINCT source AS neighbor FROM edges WHERE target = ?";\n      params.push(nodeId);\n      break;\n    case "outbound":\n      query = "SELECT DISTINCT target AS neighbor FROM edges WHERE source = ?";\n      params.push(nodeId);\n      break;\n    case "both":\n    default:\n      query = `\n        SELECT DISTINCT target AS neighbor FROM edges WHERE source = ?\n        UNION\n        SELECT DISTINCT source AS neighbor FROM edges WHERE target = ?\n      `;\n      params.push(nodeId, nodeId);\n      break;\n  }\n\n  const result = this.db.exec(query, ...params);\n  const res = result.toArray();\n  return res.map((row) => (row as { neighbor: string }).neighbor);\n}\n```\n\n#### Breadth-First Search (BFS)\n\nThe `traverse()` method implements [breadth-first search](https://en.wikipedia.org/wiki/Breadth-first_search) to explore the graph layer by layer. BFS guarantees finding the **shortest path** between two nodes.\n\nHere\'s how BFS explores a graph starting from Alice:\n\n```mermaid\ngraph TB\n    subgraph "Depth 2 (Second hop)"\n    A2[("Alice")]\n    B2[("Bob")]\n    C2[("Charlie")]\n    PX[("Project X")]\n    PY[("Project Y")]\n    A2 -.->|manages| B2\n    A2 -.->|manages| C2\n    B2 -.->|works_on| PX\n    C2 -.->|works_on| PY\n    style A2 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style B2 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style C2 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style PX fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style PY fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    end\n\n    subgraph "Depth 1 (First hop)"\n    A1[("Alice")]\n    B1[("Bob")]\n    C1[("Charlie")]\n    A1 -.->|manages| B1\n    A1 -.->|manages| C1\n    style A1 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style B1 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style C1 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    end\n\n    subgraph "Depth 0 (Start)"\n    A0[("Alice")]\n    style A0 fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    end\n```\n\nBFS visits all nodes at depth 1 before moving to depth 2. This guarantees the first path found to any node is the shortest.\n\n**Pathfinding Example**: "How is Alice connected to Project X?"\n- Depth 0: Start at Alice\n- Depth 1: Explore Bob and Charlie (Alice\'s direct reports)\n- Depth 2: Find Project X through Bob\n- Result: Alice → manages → Bob → works_on → Project X (2 hops)\n\nEven if there\'s a longer path (Alice → manages → Charlie → collaborates_with → Bob → works_on → Project X), BFS finds the shorter one first.\n\nThe traversal operates in two modes:\n\n1. **Pathfinding Mode** (when `endNodeId` is specified): Find the shortest path from start to end node\n2. **Reachability Mode** (when `endNodeId` is omitted): Find all nodes reachable within `maxDepth` hops\n\n```typescript title="src/do.ts"\ntraverse(startNodeId: string, options = {}) {\n  const { endNodeId, maxDepth = 50, direction = "both" } = options;\n  const visited = new Set<string>();\n  const queue = [{ id: startNodeId, path: [startNodeId], depth: 0 }];\n\n  while (queue.length > 0) {\n    const current = queue.shift()!;\n    if (current.depth > maxDepth) continue;\n\n    // Pathfinding: stop when target found\n    if (endNodeId && current.id === endNodeId) {\n      return { found: true, path: current.path, depth: current.depth };\n    }\n\n    if (visited.has(current.id)) continue;\n    visited.add(current.id);\n\n    // Explore neighbors\n    const neighbors = this.findNeighbors(current.id, direction);\n    for (const neighborId of neighbors) {\n      if (!visited.has(neighborId)) {\n        queue.push({\n          id: neighborId,\n          path: [...current.path, neighborId],\n          depth: current.depth + 1,\n        });\n      }\n    }\n  }\n\n  // Reachability: return all visited nodes\n  return { found: true, reachableNodes: Array.from(visited), count: visited.size };\n}\n```\n\nIn a highly connected graph (think social networks where most people are connected within 6 degrees), an unbounded traversal could visit the entire graph. By limiting depth through `maxDepth`, you can explore local neighborhoods without scanning millions of nodes.\n\nThe algorithm maintains a `visited` set to avoid cycles. Without this, you could loop indefinitely in a graph with circular references (A → B → C → A).\n\n## Building the HTTP API\n\nNow let\'s expose these operations through an HTTP API. We\'ll use Hono for routing:\n\n```typescript title="src/index.ts"\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\napp.use("/*", cors());\n\nexport default {\n  async fetch(request: Request, env: Bindings, ctx: ExecutionContext): Promise<Response> {\n    const url = new URL(request.url);\n\n    if (url.pathname.startsWith("/api/")) {\n      const apiRequest = new Request(request.url.replace("/api", ""), request);\n      return app.fetch(apiRequest, env, ctx);\n    }\n\n    return env.ASSETS.fetch(request);\n  },\n};\n\nexport { GraphDatabase } from "./do";\n\nfunction getGraphDatabaseStub(env: Bindings, graphId: string) {\n  const doId = env.GraphDatabase.idFromName(graphId);\n  return env.GraphDatabase.get(doId);\n}\n```\n\nThe routing is clean: `/api/*` goes to our Hono app, everything else is served from static assets for a simple web interface to visualise the graph.\n\nThe `getGraphDatabaseStub` function uses [`idFromName(graphId)`](https://developers.cloudflare.com/durable-objects/api/namespace/#idfromname) to get a unique ID from the graphId, and creates a [stub](https://developers.cloudflare.com/durable-objects/api/namespace/#get) which can be used to invoke methods on the associated Durable Object. This is the foundation of the multi-tenancy model:\n\n- Same input (`"user-123-graph"`) → Same Durable Object ID → Same instance\n- Different input (`"user-456-graph"`) → Different ID → Different instance\n- No coordination needed; the `idFromName` guarantees deterministic routing\n\nYou don\'t need a lookup table mapping tenant IDs to Durable Object IDs.\n\n### API Endpoints\n\nEvery endpoint extracts `graphId` from the URL path, making multi-tenancy explicit in the URL structure:\n\n```\n/api/alice-graph/nodes      # Alice\'s graph\n/api/bob-graph/nodes        # Bob\'s graph\n/api/project-x/nodes        # Project X\'s graph\n```\n\n**Node Operations:**\n- `POST /:graphId/nodes` - Create or update a node\n- `GET /:graphId/nodes/:nodeId` - Retrieve a specific node\n- `GET /:graphId/nodes?type=&limit=&offset=` - List nodes with filtering\n- `DELETE /:graphId/nodes/:nodeId` - Delete node and connected edges\n\n**Edge Operations:**\n- `POST /:graphId/edges` - Create an edge between nodes\n- `GET /:graphId/edges?nodeId=&relationship=` - Find edges by node or relationship\n- `DELETE /:graphId/edges/:source/:relationship/:target` - Remove a specific edge\n\n**Graph Operations:**\n- `GET /:graphId/nodes/:nodeId/neighbors?direction=` - Find adjacent nodes\n- `POST /:graphId/traverse` - Execute BFS pathfinding or reachability analysis\n- `GET /:graphId/stats` - Get graph metrics (node count, edge count, type distributions)\n\nEach endpoint follows the same pattern: extract the `graphId`, get the Durable Object stub, call the method, return JSON.\n\n```typescript title="src/index.ts"\napp.post("/:graphId/nodes", async (c) => {\n  const graphId = c.req.param("graphId");\n  const body = await c.req.json<InsertNode>();\n  const stub = getGraphDatabaseStub(c.env, graphId);\n  const node = await stub.addNode(body);\n  return c.json({ node });\n});\n```\n\nNo shared tables, no tenant ID columns in queries. Different graphs queries are routed to different Durable Object instances.\n\n## Interactive Graph Visualization\n\nI also vibe-coded a simple UI to visualise the state of the graph, hosted on the same Worker as the HTTP API.\n\n## Testing It Out\n\nLet\'s deploy and test our graph database using [Wrangler](https://developers.cloudflare.com/workers/wrangler/):\n\n```bash\nnpm run deploy\n```\n\nWrangler will build your TypeScript code, upload it to Cloudflare, and output your Worker URL:\n\n```\nhttps://durable-objects-graph-database.your-account.workers.dev\n```\n\n### Creating a Knowledge Graph\n\nLet\'s build a simple organizational graph with people and projects:\n\n```bash\n# Create person nodes (Alice, Bob)\ncurl -X POST https://your-worker.workers.dev/api/my-org/nodes \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "id": "alice",\n    "type": "person",\n    "data": {"name": "Alice", "role": "Engineering Manager"},\n    "userId": "system"\n  }\'\n\ncurl -X POST https://your-worker.workers.dev/api/my-org/nodes \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "id": "bob",\n    "type": "person",\n    "data": {"name": "Bob", "role": "Senior Engineer"},\n    "userId": "system"\n  }\'\n\n# Create a project node\ncurl -X POST https://your-worker.workers.dev/api/my-org/nodes \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "id": "project-x",\n    "type": "project",\n    "data": {"name": "Project X", "status": "active"},\n    "userId": "system"\n  }\'\n\n# Add relationships: Alice manages Bob, Bob works on Project X\ncurl -X POST https://your-worker.workers.dev/api/my-org/edges \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "source": "alice",\n    "target": "bob",\n    "relationship": "manages",\n    "userId": "system"\n  }\'\n\ncurl -X POST https://your-worker.workers.dev/api/my-org/edges \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "source": "bob",\n    "target": "project-x",\n    "relationship": "works_on",\n    "properties": {"hours_per_week": 40},\n    "userId": "system"\n  }\'\n```\n\n### Querying the Graph\n\nNow let\'s explore the graph:\n\n```bash\n# Find everyone Alice manages\ncurl https://your-worker.workers.dev/api/my-org/nodes/alice/neighbors?direction=outbound\n\n# Response:\n# {\n#   "neighbors": ["bob"],\n#   "count": 1\n# }\n\n# Find who\'s working on Project X\ncurl https://your-worker.workers.dev/api/my-org/nodes/project-x/neighbors?direction=inbound\n\n# Response:\n# {\n#   "neighbors": ["bob"],\n#   "count": 1\n# }\n\n# Find the path from Alice to Project X\ncurl -X POST https://your-worker.workers.dev/api/my-org/traverse \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "startNodeId": "alice",\n    "endNodeId": "project-x",\n    "maxDepth": 5\n  }\'\n\n# Response:\n# {\n#   "found": true,\n#   "path": ["alice", "bob", "project-x"],\n#   "depth": 2\n# }\n```\n\nThe traversal found a path: Alice → Bob → Project X. This shows the connection between a manager and a project through the people she manages.\n\n### Multi-Tenancy in Action\n\nCreating multiple graphs is as simple as making HTTP calls:\n\n```bash\n# Create a node in Alice\'s personal graph\ncurl -X POST https://your-worker.workers.dev/api/alice-personal/nodes \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "id": "note-1",\n    "type": "note",\n    "data": {"content": "Remember to review PRs"},\n    "userId": "alice"\n  }\'\n\n# Create a node in the company graph\ncurl -X POST https://your-worker.workers.dev/api/company-wide/nodes \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "id": "policy-1",\n    "type": "policy",\n    "data": {"title": "Remote Work Policy"},\n    "userId": "hr"\n  }\'\n```\n\nThese are completely isolated. A query on "alice-personal" will never see "company-wide" data. Each has its own Durable Object instance with its own SQLite database.\n\n## Use Cases\n\nI\'m particularly interested in AI agent uses cases.\n\n### AI Agent Memory\n\nAI agents need persistent memory to build context over time and understand complex relationships. Traditional approaches use vector databases for semantic search, but graph databases excel at representing how concepts, entities, and conversations interconnect.\n\n#### The Problem with Context-Only Memory\n\nWhen an AI agent remembers only through vector similarity, it loses the **structure** of relationships. Consider this scenario:\n\nAn AI assistant named Alice helps a user with their work. Over multiple conversations, the user mentions:\n- They\'re working on "Project X" (a web app)\n- They\'re collaborating with "Bob" (the designer)\n- "Project X" has a deadline of March 1st\n- Bob is on vacation until February 20th\n\nA vector database might retrieve "Bob" when the user asks about the deadline, but it can\'t easily answer "Who on my team is available to help before the deadline?" That requires understanding the relationships: Project X → has_deadline → March 1st, Project X → involves → Bob, Bob → unavailable_until → February 20th.\n\n#### Graph-Based Agent Memory\n\nWith a graph database, each agent builds a knowledge graph that captures not just facts, but how they relate:\n\n```mermaid\ngraph LR\n    User[("User")]\n    ProjX[("Project X - Web Redesign")]\n    Bob[("Bob - Designer")]\n    Deadline[("March 1, 2025")]\n    Vacation[("Feb 20, 2025")]\n\n    User -->|working_on| ProjX\n    User -->|collaborates_with| Bob\n    ProjX -->|involves| Bob\n    ProjX -->|deadline| Deadline\n    Bob -->|unavailable_until| Vacation\n\n    style User fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style ProjX fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style Bob fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style Deadline fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n    style Vacation fill:#E8E8E8,stroke:#AEAEAE,color:#1C1C1C\n```\n\nHere\'s how the agent builds this knowledge over time:\n\n```typescript\n// Conversation 1: User mentions they\'re working on a project\nawait fetch(\'/api/agent-alice/nodes\', {\n  method: \'POST\',\n  body: JSON.stringify({\n    id: \'project-x\',\n    type: \'project\',\n    data: { name: \'Web Redesign\', status: \'in_progress\' },\n    userId: \'agent-alice\'\n  })\n});\n\nawait fetch(\'/api/agent-alice/edges\', {\n  method: \'POST\',\n  body: JSON.stringify({\n    source: \'user\',\n    target: \'project-x\',\n    relationship: \'working_on\',\n    userId: \'agent-alice\'\n  })\n});\n\n// Conversation 2: User mentions Bob is helping with design\nawait fetch(\'/api/agent-alice/nodes\', {\n  method: \'POST\',\n  body: JSON.stringify({\n    id: \'bob\',\n    type: \'person\',\n    data: { name: \'Bob\', role: \'Designer\' },\n    userId: \'agent-alice\'\n  })\n});\n\nawait fetch(\'/api/agent-alice/edges\', {\n  method: \'POST\',\n  body: JSON.stringify({\n    source: \'project-x\',\n    target: \'bob\',\n    relationship: \'involves\',\n    userId: \'agent-alice\'\n  })\n});\n\n// Conversation 3: User mentions Bob\'s vacation\nawait fetch(\'/api/agent-alice/edges\', {\n  method: \'POST\',\n  body: JSON.stringify({\n    source: \'bob\',\n    target: \'unavailable-period\',\n    relationship: \'unavailable_until\',\n    properties: { date: \'2025-02-20\' },\n    userId: \'agent-alice\'\n  })\n});\n```\n\nNow when the user asks "Is my team available to help me finish before the deadline?", the agent can traverse the graph:\n\n```typescript\n// Start from the user, explore their projects and collaborators\nconst context = await fetch(\'/api/agent-alice/traverse\', {\n  method: \'POST\',\n  body: JSON.stringify({\n    startNodeId: \'user\',\n    maxDepth: 3,  // Explore up to 3 hops away\n    direction: \'both\'\n  })\n});\n\n// Returns: User → Project X → Bob → Unavailable Until Feb 20\n// Agent can reason: "Bob won\'t be available until Feb 20, but your deadline is March 1,\n// so you\'ll have 9 days to work together"\n```\n\n#### Why This Works\n\nEach agent gets its own graph (`/api/agent-alice/`, `/api/agent-bob/`). As the agent has more conversations, the graph grows organically:\n\n- **Conversations** link to **Topics** they discuss\n- **Topics** link to **Projects**, **People**, **Deadlines**\n- **Projects** link to **Tasks**, **Documents**, **Decisions**\n- **People** link to **Skills**, **Availability**, **Preferences**\n\nThe graph becomes a structure memory layer that captures not just what the agent knows, but how concepts interconnect. When recalling context, traversal provides the most relevant connected information, not just similar words, but structurally related concepts.\n\n### Graph-RAG Systems\n\nRetrieval-Augmented Generation (RAG) enhances LLMs by providing relevant context from external knowledge bases. Graph-based RAG goes beyond simple vector similarity by modeling relationships between documents, entities, and concepts. Store document chunks and their relationships for more intelligent retrieval. \n\n## Limitations and Considerations\n\nWhile this pattern is powerful, there are important limitations to understand:\n\n1. **Scale per Graph**: Each Durable Object instance is limited to 10 GB, as such is best suited for graphs with thousands to tens of thousands of nodes. For larger graphs, consider sharding.\n\n2. **Cross-Graph Queries**: You can\'t easily query across multiple graphs. Each Durable Object is isolated. If you need to aggregate data from multiple tenants, you\'ll need to query each graph individually.\n\n3. **Cold Start Latency**: The first request to a new Durable Object has some initialization overhead. For frequently accessed graphs this isn\'t an issue, but rarely-used graphs might experience slightly higher latency on first access.\n\n4. **Global Consistency**: Durable Objects provide strong consistency within a single instance but don\'t offer distributed transactions across instances. If you need atomicity across multiple graphs, you\'ll need to implement your own coordination.\n\n5. **Backup and Export**: There\'s no built-in backup system. For production use, you\'ll want to implement periodic exports of graph data to durable storage like [Cloudflare R2](https://developers.cloudflare.com/r2/).\n\n## What\'s Next\n\nThis foundation enables many extensions:\n\n- **Full-Text Search**: Add [SQLite FTS5](https://www.sqlite.org/fts5.html) virtual tables for text search within node and edge properties\n\n- **Complex nested queries**: Add the ability to run more complex queries on the JSON data of nodes and edges using [SQLite json_tree](https://sqlite.org/json1.html#jtree) functions\n\n- **Graph Algorithms**: Implement [PageRank](https://en.wikipedia.org/wiki/PageRank), [community detection](https://en.wikipedia.org/wiki/Community_structure), or [centrality metrics](https://en.wikipedia.org/wiki/Centrality) for graph analysis\n\n- **Graph Embeddings**: Store vector embeddings on nodes using [Cloudflare Vectorize](https://developers.cloudflare.com/vectorize/) for hybrid graph + semantic similarity search\n\n- **Batch Operations**: Add endpoints for bulk node/edge creation and updates to improve performance for large imports\n\nThe complete source code with a visualization UI is available on [GitHub](https://github.com/boristane/cloudflare-dev-101/tree/main/durable-objects-graph-database). Fork it and build something interesting.\n\n## Conclusion\n\nDurable Objects flip the traditional database model on its head. Instead of managing infrastructure and partitioning data, you create instances on demand. Each instance is a complete, isolated database with its own storage and compute.\n\nWith SQLite storage, migrations, and proper schema design, each instance becomes a fully-featured database that happens to run at the edge.\n\nThis pattern works for graphs, but it generalizes to any domain where you need isolated per-tenant data stores. Document databases, time-series data, etc; the architecture is the same.',
    },
    {
        "title": "Context engineering is what makes AI magical",
        "link": "https://boristane.com/blog/context-engineering/",
        "guid": "https://boristane.com/blog/context-engineering/",
        "description": "What's context engineering and why you should spend more time on it.",
        "pubDate": "Sun, 22 Jun 2025 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "Everyone's busy tweaking prompts, swapping models, chaining tools. Yeah, models are getting better. Tools are getting fancier. But none of that matters if your **context sucks**.\n\nHere's the uncomfortable truth: **the main thing that really matters when building AI agents is the quality of the context you give the model.** Models are so good that most SOTA models can do most things with outstanding quality.\n\nTwo products could be doing the exact same thing, but one feels magical and the other feels like a cheap demo. The difference? Context.\n\n---\n\n## What is context?\n\nContext is everything the model sees before it produces tokens. It includes:\n\n* the **system prompt** (what you tell the model it is)\n* the **user message** (what the user is asking)\n* any **external information**, tools, or retrieved documents you stuff in before the call\n* implicit context like **who the user is**, what they've done before, what they want right now, etc.\n\nThe better this context, the better the model performs. Garbage in, garbage out, still true in 2025.\n\n---\n\n## Context > Model\n\nModels are already better than most of us at most tasks. But most AI tools still underperform, not because the model is bad, but because we're feeding it a half-baked view of the world.\n\nLet's look at RAG as an example.\n\nNaive RAG just dumps the top 3 chunks into the prompt and hopes for the best. Useful, sometimes. But the moment you move beyond toy examples, this starts to fall apart.\n\nAgentic RAG, builds a contextual snapshot that includes data from multiple sources:\n\n* the question\n* related documents\n* source structure\n* metadata\n* and critically, **the user's intent and environment**\n\nFor example, a coding agent shouldn't just embed source files and search them. It should know:\n\n* how to find and read any file in your repository\n* which files were changed recently\n* which files are open in your IDE\n* what the LSP says about types and errors\n* even what production logs and metrics say\n\nThat's context. And that's the difference between “meh” and “wow”.\n\n---\n\n## Email example: the magic of good context\n\nLet's say you're a CTO at a startup. You get an email:\n\n> Hey Boris, just checking if you're around for a quick sync tomorrow. Would love to chat through a few ideas from our last call.\n\nA decent AI email tool will reply like:\n\n> Dear Jim,\n\n> Thank you for your message. Tomorrow works for me. May I ask what time you had in mind?\n\n> Best regards,\n> Boris\n\n**Who writes emails like this?**\n\nA magical agent will do a few things first:\n\n* check your **calendar**: you're in back-to-back calls all day.\n* look at **previous emails to this person**: you're friendly, informal.\n* scan **recent meeting notes**: this person is pitching a joint partnership.\n* pull in your **contact list**: they're a senior PM at a partner org.\n* apply your **system prompt customisation**: \"be concise, decisive, warm\"\n\nAnd finally generate and autonomously send an email that actually is helpful:\n\n> *Hey Jim! Tomorrow's packed on my end, back-to-back all day. Thursday AM free if that works for you? Sent an invite, lmk if works.*\n\nThat's magic. Not because the model is smarter, but because the context is richer.\n\n---\n\n## Build context like it's your product\n\nYou wouldn't build a product without thinking deeply about state, user intent, and interaction history. So why treat your AI agent like a stateless chatbot?\n\nContext engineering is the new prompt engineering.\n\nIt means:\n\n* Designing the right structure and format for context\n* Knowing what context actually helps the model perform\n* Building pipelines to fetch, transform and deliver this context at runtime\n* Constantly improving context quality with feedback loops\n\n---\n\n## TL;DR\n\n* Models are great.\n* But **context is king**.\n* Context is the difference between a dumb assistant and a superpowered teammate.\n* Build your agents like you build your products: obsess over what they know, when they know it, and how they use it.\n\nIf your agent isn't magical yet, don't swap the model, **fix your context.**",
    },
    {
        "title": "Contextual Retrieval-Augmented Generation (RAG) on Cloudflare Workers",
        "link": "https://boristane.com/blog/cloudflare-contextual-rag/",
        "guid": "https://boristane.com/blog/cloudflare-contextual-rag/",
        "description": "Implementing Contextual Retrieval-Augmented Generation with Using Cloudflare AI, Vectorize and D1 database with Drizzle ORM in a Hono app",
        "pubDate": "Sat, 26 Apr 2025 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": 'Today I am sharing a powerful pattern I\'ve implemented on the [Cloudflare Developers Platform](https://workers.dev) whilst building my [weekends project](https://basebrain.ai): Contextual Retrieval-Augmented Generation (RAG).\n\nTraditional RAG implementations often fall short when presented with nuanced queries or complex documents. Anthropic introduced [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) as an answer to this problem.\n\nIn this blog post, I\'ll walk through implementing a contextual RAG system entirely on the [Cloudflare Developer Plarform](https://workers.dev). You\'ll see how combining vector search with full-text search, query rewriting, and smart ranking techniques creates a system that truly understands your content.\n\n## What is Contextual RAG\n\nIf you\'re doing any kind of RAG today, you\'re probably:\n\n1. Chunking documents\n2. Storing vectors\n3. Searching embeddings\n4. Stuffing the results into your LLM context window\n\nThe problem is simple: chunking is often not good enough. When you retrieve a chunk based on cosine similarity, it might be totally out of context and irrelevant to the user query.\n\nIn a nutshell, key issues with traditional RAG systems are:\n\n1. **Context loss**: When documents are chunked into smaller pieces, the broader context that gives meaning to each chunk is often lost\n2. **Relevance issues**: Vector search alone might retrieve semantically similar content that isn\'t actually relevant to the query\n3. **Query limitations**: User queries are often ambiguous or lack specificity needed for effective retrieval\n\nContextual RAG provides a solution to these issues. It consists of pre-pending every chunk with a short explanation situating it within the context of the entire document. Every chunk is passed through an LLM to provide this context.\n\nThe prompt looks like:\n\n```txt\n<document> \n{{WHOLE_DOCUMENT}} \n</document> \nHere is the chunk we want to situate within the whole document \n<chunk> \n{{CHUNK_CONTENT}} \n</chunk> \nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. \n```\n\nThe result is a short sentence situating the chunk within the context of the entire documents. It is prepended to the actual chunk before inserting it into the vector databse. This provides a solution to the context loss challenge.\n\nAdditionally, the query side of the RAG system is enhanced with full-text search, reciprocal rank fusion and a LLM reranker.\n\nWith this technique the challenges of traditional RAG are addressed by:\n\n1. **Enhancing chunks with context**: Each chunk is augmented with contextual information from the full document\n2. **Using hybrid search**: Combining vector similarity with keyword-based search\n3. **Rewriting queries**: Expanding and transforming user queries into multiple search variations\n4. **Intelligent ranking**: Using sophisticated algorithms to merge and rank results from different search methods\n\nLet\'s dive deeper and see how to implemented this on Cloudflare Workers.\n\n## Project Overview\n\nWe\'ll be using:\n\n- [**Cloudflare Workers**](https://developers.cloudflare.com/workers/) - Our serverless platform\n- [**Cloudflare D1**](https://developers.cloudflare.com/d1/) - SQL database for document storage\n- [**Cloudflare Vectorize**](https://developers.cloudflare.com/vectorize/) - Vector search engine\n- [**Workers AI**](https://developers.cloudflare.com/ai/) - Cloudflare AI platform\n- [**Drizzle ORM**](https://orm.drizzle.team/) - Type-safe database access\n- [**Hono**](https://hono.dev/) - Lightweight framework for our API routes\n\nHere\'s the basic structure of the project:\n\n```\n.\n├── bindings.ts                # Bindings TypeScript Definition\n├── bootstrap.sh               # Automate creating the Cloudflare resources\n├── drizzle.config.ts          # Database configuration\n├── fts5.sql                   # Full-text search SQL triggers\n├── package.json               # Dependencies and scripts\n├── src\n│   ├── db\n│   │   ├── index.ts           # Database CRUD operations\n│   │   ├── schemas.ts         # Databae schema definition\n│   │   └── types.ts           # Database TypeScript types\n│   ├── index.ts               # Main Worker code and Durable Object\n│   ├── search.ts              # Search functionality\n│   ├── utils.ts               # Util functions\n│   └── vectorize.ts           # Vectorize operations\n├── tsconfig.json              # TypeScript configuration\n└── wrangler.json              # Cloudflare Workers configuration\n```\n\nThis final solution will be implementing the following architecture. The goal of this blog post is to walk through each item individually and finally explain how they all work together.\n\nThe complete source code for this blog post is available on [GitHub](https://github.com/boristane/cloudflare-dev-101/tree/main/contextual-rag). I suggest following the post with the source code available for reference.\n\n## Bootstrapping the RAG System\n\nWe first need to create our cloud resources. Let\'s create a `bootstrap.sh` script that creates a Vectorize index and a D1 database:\n\n```bash title="bootstrap.sh"\n#! /bin/bash\nset -e\n\nnpx wrangler vectorize create contextual-rag-index --dimensions=1024 --metric=cosine\nnpx wrangler vectorize create-metadata-index contextual-rag-index --property-name=timestamp --type=number\n\nnpx wrangler d1 create contextual-rag\n```\n\nIt use [wrangler](https://developers.cloudflare.com/workers/wrangler/) to create:\n\n1. A Vectorize index for storing embeddings\n2. A metadata index for timestamp-based filtering\n3. A D1 database for document storage\n\nRunning this script will output a database ID we will use in our `wrangler.json` and `drizzle.config.ts` files.\n\nNext, after creating a Node.js project with `npm` and configuring TypeScript, let\'s create our `wrangler.json` file to configure our Cloudflare Worker and connect our resources to it.\n\n```json title="wrangler.json"\n{\n  "name": "contextual-rag",\n  "compatibility_date": "2024-11-12",\n  "workers_dev": true,\n  "upload_source_maps": true,\n  "observability": {\n    "enabled": true\n  },\n  "main": "./src/index.ts",\n  "vectorize": [\n    {\n      "binding": "VECTORIZE",\n      "index_name": "contextual-rag-index"\n    }\n  ],\n  "d1_databases": [\n    {\n      "binding": "D1",\n      "database_name": "contextual-rag",\n      "database_id": "<REPLACE WITH YOUR DATABASE ID>"\n    }\n  ],\n  "ai": {\n    "binding": "AI"\n  },\n  "vars": {}\n}\n```\n\n## Document Ingestion Pipeline\n\nAfter initializing the project, the first step is to ingest documents we want to retrieve. We are building everything on a Hono application deployed on Cloudflare Workers.\n\nOur API endpoint for document uploads looks like this:\n\n```typescript title="src/index.ts"\napp.post(\'/\', async (c) => {\n  const { contents } = await c.req.json();\n  if (!contents || typeof contents !== "string") return c.json({ message: "Bad Request" }, 400)\n\n  const doc = await createDoc(c.env, { contents });\n\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: 1024,\n    chunkOverlap: 200,\n  });\n  const raw = await splitter.splitText(contents);\n  const chunks = await contextualizeChunks(c.env, contents, raw)\n  await insertChunkVectors(c.env, { docId: doc.id, created: doc.created }, chunks);\n\n  return c.json(doc)\n});\n```\n\nIt\'s a `POST` endpoint that takes the contents of the document to process. The processing steps are:\n\n1. Store the complete document in the database\n2. Split the document into manageable chunks\n3. Contextualize each chunk\n4. Generate vector embeddings for each chunk\n5. Store the chunks and their embeddings\n\n### Text Splitting\n\nWe split the documents because LLMs have limited context windows, and retrieving entire documents for every query would be inefficient. By splitting documents into smaller chunks, we can retrieve only the most relevant pieces.\n\nWe use the [RecursiveCharacterTextSplitter](https://v03.api.js.langchain.com/classes/langchain.text_splitter.RecursiveCharacterTextSplitter.html) from LangChain, which intelligently splits text based on content boundaries:\n\n```typescript title="src/index.ts"\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1024,\n  chunkOverlap: 200,\n});\nconst raw = await splitter.splitText(contents);\n```\n\nThe `chunkSize` parameter controls the maximum size of each chunk, while `chunkOverlap` creates an overlap between adjacent chunks. This overlap helps maintain context across chunk boundaries and prevents information from being lost at the dividing points.\n\n### Context Enhancement\n\nThis is where we do the "contextual" part of Contextual RAG. Instead of storing raw chunks, we first enhance each chunk with contextual information situating it within the context of the full text:\n\n```typescript title="src/vectorize.ts"\nexport async function contextualizeChunks(env: { AI: Ai }, content: string, chunks: string[]): Promise<string[]> {\n  const promises = chunks.map(async c => {\n\n    const prompt = `<document> \n${content} \n</document> \nHere is the chunk we want to situate within the whole document \n<chunk> \n${c}\n</chunk> \nPlease give a short succinct context to situate this chunk within the overall\ndocument for the purposes of improving search retrieval of the chunk.\nAnswer only with the succinct context and nothing else. `;\n\n    // @ts-ignore\n    const res = await env.AI.run("@cf/meta/llama-3.1-8b-instruct-fast", {\n      prompt,\n    }) as { response: string }\n\n    return `${res.response}; ${c}`;\n  })\n\n  return await Promise.all(promises);\n}\n```\nWithout the context, chunks become isolated islands of information, divorced from their surrounding context. For example, a chunk containing "it increases efficiency by 40%" is meaningless without knowing what "it" refers to. By adding contextual information to each chunk, we make them more self-contained and improve retrieval accuracy.\n\nWe use an LLM to analyze the relationship between the chunk and the full document, then generate a short context summary that precedes the chunk text. This enhancement makes each chunk more self-contained and improves retrieval relevance.\n\nFor example, I tried this with a short text describing Paris. After chunking the text, one of the chunks was:\n\n> The city doesn’t shout. It smirks. It moves with a kind of practiced nonchalance, a shrug that says, of course it’s beautiful here. It’s a city built for daydreams and contradictions, grand boulevards designed for kings now flooded with delivery bikes and tourists holding maps upside-down. Crumbling stone facades wear ivy and grime like couture. Every corner feels staged, but somehow still effortless, like the city isn’t even trying to impress you.\n\nThere is no indication that this chunk is talking about Paris.\n\nThe enhanced chunk after running through the LLM is:\n\n> The chunk describes the essence and atmosphere of Paris, highlighting its unique blend of beauty, history, and contradictions.; The city doesn’t shout. It smirks. It moves with a kind of practiced nonchalance, a shrug that says, of course it’s beautiful here. It’s a city built for daydreams and contradictions, grand boulevards designed for kings now flooded with delivery bikes and tourists holding maps upside-down. Crumbling stone facades wear ivy and grime like couture. Every corner feels staged, but somehow still effortless, like the city isn’t even trying to impress you.\n\nA retrieval query consisting of just the word **Paris** is more likely to match the enhanced chunk versus the raw chunk without context.\n\n### Vector Generation and Storage\n\nAfter contextualizing the chunks, we generate vector embeddings and store them in Cloudflare Vectorize and D1:\n\n```typescript title="src/vectorize.ts"\nexport async function insertChunkVectors(\n  env: { D1: D1Database, AI: Ai, VECTORIZE: Vectorize },\n  data: { docId: string, created: Date },\n  chunks: string[],\n) {\n\n  const { docId, created } = data;\n  const batchSize = 10;\n  const insertPromises = [];\n\n  for (let i = 0; i < chunks.length; i += batchSize) {\n    const chunkBatch = chunks.slice(i, i + batchSize);\n\n    insertPromises.push(\n      (async () => {\n        const embeddingResult = await env.AI.run("@cf/baai/bge-large-en-v1.5", {\n          text: chunkBatch,\n        });\n        const embeddingBatch: number[][] = embeddingResult.data;\n\n        const chunkInsertResults = await Promise.all(chunkBatch.map(c => createChunk(env, { docId, text: c })))\n        const chunkIds = chunkInsertResults.map((result) => result.id);\n\n        await env.VECTORIZE.insert(\n          embeddingBatch.map((embedding, index) => ({\n            id: chunkIds[index],\n            values: embedding,\n            metadata: {\n              docId,\n              chunkId: chunkIds[index],\n              text: chunkBatch[index],\n              timestamp: created.getTime(),\n            },\n          }))\n        );\n      })()\n    );\n  }\n\n  await Promise.all(insertPromises);\n}\n```\n\nThis function:\n\n1. Divides chunks into batches of 10\n2. Generates embeddings for each batch using Cloudflare AI\n3. Stores the chunks in the D1 database\n4. Stores the embeddings and associated metadata in Vectorize\n\nThe `createChunk` function inserts the chunks the D1 database and gives us a unique ID for each chunk, which is then shared in Vectorize. The next section gives more details on the D1 database.\n\nThe metadata attached to each vector includes the original text, document ID, and timestamp, which enables filtering and improves retrieval performance.\n\n### D1 Database Schema\n\nOur RAG system must combine full-text and vector search.\n\nWe have two data models to store outside of the vector database: full documents and the chunks derived from them. For this, I\'m using Cloudflare D1 (SQLite) with Drizzle ORM.\n\nHere\'s the schema definition:\n\n```typescript title="src/db/schemas.ts"\n\nexport const docs = sqliteTable(\n  "docs",\n  {\n    id: text("id")\n      .notNull()\n      .primaryKey()\n      .$defaultFn(() => randomString()),\n    contents: text("contents"),\n    created: integer("created", { mode: "timestamp_ms" })\n      .$defaultFn(() => new Date())\n      .notNull(),\n    updated: integer("updated", { mode: "timestamp_ms" })\n      .$onUpdate(() => new Date())\n      .notNull(),\n  },\n  (table) => ([\n    index("docs.created.idx").on(table.created),\n  ]),\n);\n\nexport const chunks = sqliteTable(\n  "chunks",\n  {\n    id: text("id")\n      .notNull()\n      .primaryKey()\n      .$defaultFn(() => randomString()),\n    docId: text(\'doc_id\').notNull(),\n    text: text("text").notNull(),\n    created: integer("created", { mode: "timestamp_ms" })\n      .$defaultFn(() => new Date())\n      .notNull(),\n  },\n  (table) => ([\n    index("chunks.doc_id.idx").on(table.docId),\n  ]),\n);\n\nfunction randomString(length = 16): string {\n  const chars = "abcdefghijklmnopqrstuvwxyz";\n  const resultArray = new Array(length);\n\n  for (let i = 0; i < length; i++) {\n    const randomIndex = Math.floor(Math.random() * chars.length);\n    resultArray[i] = chars[randomIndex];\n  }\n\n  return resultArray.join("");\n}\n```\n\nWith these schemas, we can add CRUD functions to write reand and write documents and chunks in D1.\n\n```typescript title="src/db/index.ts"\n\nexport function getDrizzleClient(env: DB) {\n  return drizzle(env.D1, {\n    schema,\n  });\n}\n\nexport async function createDoc(env: DB, doc: InsertDoc): Promise<Doc> {\n  const d1 = getDrizzleClient(env);\n\n  const [res] = await d1\n    .insert(docs)\n    .values(doc)\n    .onConflictDoUpdate({\n      target: [docs.id],\n      set: doc,\n    })\n    .returning();\n\n  return res;\n}\n\nexport async function listDocsByIds(\n  env: DB,\n  params: { ids: string[] },\n): Promise<Doc[]> {\n  const d1 = getDrizzleClient(env);\n\n  const qs = await d1\n    .select()\n    .from(docs)\n    .where(inArray(docs.id, params.ids))\n  return qs;\n}\n\nexport async function createChunk(env: DB, chunk: InsertChunk): Promise<Chunk> {\n  const d1 = getDrizzleClient(env);\n\n  const [res] = await d1\n    .insert(chunks)\n    .values(chunk)\n    .onConflictDoUpdate({\n      target: [chunks.id],\n      set: chunk,\n    })\n    .returning();\n\n  return res;\n}\n```\n\n### SQLite Full-Text Search\n\nWhile vector search excels at semantic similarity, it often misses exact keyword matches that full-text search handles perfectly. Full-text search excels at finding exact keyword matches and can retrieve relevant content even when the semantic meaning might be ambiguous. By combining both approaches, we get the best of both worlds.\n\nSQLite provides a powerful full-text search extension called [FTS5](https://www.sqlite.org/fts5.html). We\'ll create a virtual table that mirrors our `chunks` table but with full-text search capabilities:\n\n```sql title="fts5.sql"\nCREATE VIRTUAL TABLE chunks_fts USING fts5(\n\tid UNINDEXED,\n\tdoc_id UNINDEXED,\n\ttext,\n\tcontent = \'chunks\',\n\tcreated\n);\n\nCREATE TRIGGER chunks_ai\nAFTER\nINSERT\n\tON chunks BEGIN\nINSERT INTO\n\tchunks_fts(id, doc_id, text, created)\nVALUES\n\t(\n\t\tnew.id,\n\t\tnew.doc_id,\n\t\tnew.text,\n\t\tnew.created\n\t);\n\nEND;\n\nCREATE TRIGGER chunks_ad \nAFTER DELETE ON chunks \nFOR EACH ROW\nBEGIN\n    DELETE FROM chunks_fts WHERE id = old.id;\n\t\tINSERT INTO chunks_fts(chunks_fts) VALUES(\'rebuild\');\nEND;\n\nCREATE TRIGGER chunks_au \nAFTER UPDATE ON chunks BEGIN\n    DELETE FROM chunks_fts WHERE id = old.id;\n    INSERT INTO chunks_fts(id, doc_id, text, created)\n    VALUES (new.id, new.doc_id, new.text, new.created);\n\t\tINSERT INTO chunks_fts(chunks_fts) VALUES(\'rebuild\');\nEND;\n```\n\nThe triggers ensure that our FTS table stays in sync with the main chunks table. Whenever a chunk is inserted, updated, or deleted, the corresponding entry in the FTS table is also modified.\n\nWe use [DrizzleKit](https://orm.drizzle.team/docs/kit-overview) to create migration files for our D1 database with this schema.\n\n```bash\ndrizzle-kit generate\n```\n\nThe above SQL for the FTS tables must be included manually in the migration file before running the migration in D1. Otherwise, we will not have full-text search capabilities in our SQLite database.\n\n```bash\ndrizzle-kit migrate\n```\n\nOnce the migration is completed, we\'re ready to start processing documets in the ingestion side of our RAG system.\n\n## Query Processing and Search\n\nNow let\'s examine how queries are handled. Our query endpoint looks like this:\n\n```typescript title="src/index.ts"\napp.post(\'/query\', async (c) => {\n  const { prompt, timeframe } = await c.req.json();\n\n  if (!prompt) return c.json({ message: "Bad Request" }, 400);\n\n  const searchOptions: {\n    timeframe?: { from?: number; to?: number };\n  } = {};\n\n  if (timeframe) {\n    searchOptions.timeframe = timeframe;\n  }\n\n  const ai = createWorkersAI({ binding: c.env.AI });\n  // @ts-ignore\n  const model = ai("@cf/meta/llama-3.1-8b-instruct-fast") as LanguageModel;\n\n  const { queries, keywords } = await rewriteToQueries(model, { prompt });\n\n  const { chunks } = await searchDocs(c.env, {\n    questions: queries,\n    query: prompt,\n    keywords,\n    topK: 8,\n    scoreThreshold: 0.501,\n    ...searchOptions,\n  });\n\n  const uniques = getUniqueListBy(chunks, "docId").map((r) => {\n    const arr = chunks\n      .filter((f) => f.docId === r.docId)\n      .map((v) => v.score);\n    return {\n      id: r.docId,\n      score: Math.max(...arr),\n    };\n  });\n\n  const res = await listDocsByIds(c.env, { ids: uniques.map(u => u.id) });\n  const answer = await c.env.AI.run("@cf/meta/llama-3.3-70b-instruct-fp8-fast", {\n    prompt: `${prompt}\n    \nContext: ${chunks}`\n  })\n\n  return c.json({\n    keywords,\n    queries,\n    chunks,\n    answer,\n    docs: res.map(doc => ({ ...doc, score: uniques.find(u => u.id === doc.id)?.score || 0 })).sort((a, b) => b.score - a.score)\n  })\n});\n\nfunction getUniqueListBy<T extends Record<string, unknown>>(arr: T[], key: keyof T): T[] {\n  const result: T[] = [];\n  for (const elt of arr) {\n    const found = result.find((t) => t[key] === elt[key]);\n    if (!found) {\n      result.push(elt);\n    }\n  }\n  return result;\n}\n```\n\nIt\'s a `POST` endpoint that uses the [AI SDK](https://sdk.vercel.ai/) and the [Workers AI Provider](https://github.com/cloudflare/ai/tree/main/packages/workers-ai-provider) to complete the following steps:\n\n1. **Query rewriting**: Rewrite the user prompt into multiple related questions and keywords to improve RAG performance \n2. **Hybrid search**: Combining vector and text search\n3. **Result fusion and reranking**\n4. **Answer generation**\n\nLet\'s examine each in detail.\n\n### Query Rewriting\n\nUsers rarely express their needs perfectly on the first try. Their queries are often ambiguous or lack specific keywords that would make retrieval effective. Query rewriting expands the original query into multiple variations:\n\n```typescript title="src/search.ts"\nexport async function rewriteToQueries(model: LanguageModel, params: { prompt: string }): Promise<{ keywords: string[], queries: string[] }> {\n  const prompt = `Given the following user message,\nrewrite it into 5 distinct queries that could be used to search for relevant information,\nand provide additional keywords related to the query.\nEach query should focus on different aspects or potential interpretations of the original message.\nEach keyword should be a derived from an interpratation of the provided user message.\n\nUser message: ${params.prompt}`;\n\n  try {\n    const res = await generateObject({\n      model,\n      prompt,\n      schema: z.object({\n        queries: z.array(z.string()).describe(\n          "Similar queries to the user\'s query. Be concise but comprehensive."\n        ),\n        keywords: z.array(z.string()).describe(\n          "Keywords from the query to use for full-text search"\n        ),\n      }),\n    })\n\n    return res.object;\n  } catch (err) {\n    return {\n      queries: [params.prompt],\n      keywords: []\n    }\n  }\n}\n```\n\nBy generating multiple interpretations of the original query, we can capture different aspects and increase the likelihood of finding relevant information.\n\nThe AI model generates:\n1. A set of expanded queries that explore different interpretations of the user\'s question\n2. Keywords extracted from the query for full-text search\n\nThis approach dramatically improves search recall compared to using just the original query.\n\nFor example, I sent this prompt:\n\n> what\'s special about paris?\n\nand it was rewritten as:\n\n```json\n{\n  "keywords": [\n\t\t"paris",\n\t\t"eiffel tower",\n\t\t"monet",\n\t\t"loire river",\n\t\t"montmartre",\n\t\t"notre dame",\n\t\t"art",\n\t\t"history",\n\t\t"culture",\n\t\t"tourism"\n\t],\n\t"queries": [\n\t\t"paris attractions",\n\t\t"paris landmarks",\n\t\t"paris history",\n\t\t"paris culture",\n\t\t"paris tourism"\n\t]\n}\n```\n\nBy searching the databases with all these combinations, we increase the likelihood of finding relevant chunks.\n\n### Hybrid Search\n\nWith our rewritten queries and extracted keywords, we now perform a hybrid search using both vector similarity and full-text search:\n\n```typescript title="src/search.ts"\nexport async function searchDocs(env: SearchBindings, params: DocSearchParams): Promise<{ chunks: Array<{ text: string, id: string, docId: string; score: number }> }> {\n  const { timeframe, questions, keywords } = params;\n\n  const [vectors, sql] = (await Promise.all([\n    queryChunkVectors(env, { queries: questions, timeframe, },),\n    searchChunks(env, { needles: keywords, timeframe }),\n  ]));\n\n  const searchResults = {\n    vectors,\n    sql: sql.map(item => {\n      return {\n        id: item.id,\n        text: item.text,\n        docId: item.doc_id,\n        rank: item.rank,\n      }\n    })\n  };\n\n  const mergedResults = performReciprocalRankFusion(searchResults.sql, searchResults.vectors);\n  const res = await processSearchResults(env, params, mergedResults);\n  return res;\n}\n```\n\nVector search excels at finding semantically similar content but can miss exact keyword matches while full-text search is great at finding keyword matches but lacks semantic understanding.\n\nBy running both search types in parallel and then merging the results, we get more comprehensive coverage.\n\nThe vector search looks for embeddings similar to our query embeddings, applying timestamp filters if they are provided:\n\n```typescript title="src/vectorize.ts"\nexport async function queryChunkVectors(\n  env: { AI: Ai, VECTORIZE: Vectorize },\n  params: { queries: string[], timeframe?: { from?: number, to?: number } }) {\n  const { queries, timeframe, } = params;\n  const queryVectors = await Promise.all(\n    queries.map((q) => env.AI.run("@cf/baai/bge-large-en-v1.5", { text: [q] }))\n  );\n\n  const filter: VectorizeVectorMetadataFilter = {  };\n  if (timeframe?.from) {\n    // @ts-expect-error error in the package\n    filter.timestamp = { "$gte": timeframe.from }\n  }\n  if (timeframe?.to) {\n    // @ts-expect-error error in the package\n    filter.timestamp = { "$lt": timeframe.to }\n  }\n\n  const results = await Promise.all(\n    queryVectors.map((qv) =>\n      env.VECTORIZE.query(qv.data[0], {\n        topK: 20,\n        returnValues: false,\n        returnMetadata: "all",\n        filter,\n      })\n    )\n  );\n\n  return results;\n}\n```\n\nThe full-text search uses SQLite\'s FTS5 to find keyword matches:\n\n```typescript title="src/db/index.ts"\nexport async function searchChunks(env: DB, params: { needles: string[], timeframe?: { from?: number, to?: number }; }, limit = 40) {\n  const d1 = getDrizzleClient(env);\n\n  const { needles, timeframe } = params;\n  const queries = needles.filter(Boolean).map(\n    (term) => {\n      const sanitizedTerm = term.trim().replace(/[^\\w\\s]/g, \'\');\n\n      return `\n        SELECT chunks.*, bm25(chunks_fts) AS rank\n        FROM chunks_fts\n        JOIN chunks ON chunks_fts.id = chunks.id\n        WHERE chunks_fts MATCH \'${sanitizedTerm}\'\n        ${timeframe?.from ? `AND created > ${timeframe.from}` : \'\'}\n        ${timeframe?.to ? `AND created < ${timeframe.to}` : \'\'}\n        ORDER BY rank\n        LIMIT ${limit}\n      `;\n    }\n  );\n\n  const results = await Promise.all(\n    queries.map(async (query) => {\n      const res = await d1.run(query);\n      return res.results as ChunkSearch[];\n    })\n  );\n\n  return results.flat()\n}\n```\n\nWe\'re using the [BM25 ranking algorithm](https://www.sqlite.org/fts5.html#the_bm25_function) (built into FTS5) to sort results by relevance. It\'s an algorithm that considers term frequency, document length, and other factors to determine relevance.\n\n### Reciprocal Rank Fusion\n\nAfter getting results from both search methods, we need to merge them. This is where [Reciprocal Rank Fusion (RRF)](https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking) comes in.\n\nIt\'s a rank aggregation method that combines ranking from multiple sources into a single unified ranking.\n\nWhen you have multiple ranked lists from different systems, each with their own scoring method, it\'s challenging to merge them fairly. RRF provides a principled way to combine these lists by:\n\n1. Considering the rank position rather than raw scores (which may not be comparable)\n2. Giving higher weights to items that appear at high ranks in multiple lists\n3. Using a constant `k` to mitigate the impact of outliers\n\nThe formula gives each item a score based on its rank in each list: `score = 1 / (k + rank)`. Items that appear high in multiple lists get the highest combined scores.\n\nThe constant `k` (set to 60 in our implementation) is crucial as it prevents items that appear at the very top of only one list from completely dominating the results. A larger k value makes the algorithm more conservative, reducing the advantage of top-ranked items and giving more consideration to items further down the lists.\n\n```typescript title="src/utils.ts"\nexport function performReciprocalRankFusion(\n  fullTextResults: DocMatch[],\n  vectorResults: VectorizeMatches[]\n): { docId: string,  id: string; score: number; text?: string }[] {\n\n  const vectors = uniqueVectorMatches(vectorResults.flatMap(r => r.matches));\n  const sql = uniqueDocMatches(fullTextResults);\n\n  const k = 60; // Constant for fusion, can be adjusted\n  const scores: { [key: string]: { id: string, text?: string;  docId: string, score: number } } = {};\n\n  // Process full-text search results\n  sql.forEach((result, index) => {\n    const key = result.id;\n    const score = 1 / (k + index);\n    scores[key] = {\n      id: result.id,\n      docId: result.docId,\n      text: result.text,\n      score: (scores[key]?.score || 0) + score,\n    };\n  });\n\n  // Process vector search results\n  vectors.forEach((match, index) => {\n    const key = match.id;\n    const score = 1 / (k + index);\n    scores[key] = {\n      id: match.id,\n      docId: match.metadata?.docId as string,\n      text: match.metadata?.text as string,\n      score: (scores[key]?.score || 0) + score,\n    };\n  });\n\n  const res = Object.entries(scores)\n    .map(([key, { id, score, docId, text }]) => ({ docId, id, score, text }))\n    .sort((a, b) => b?.score - a?.score);\n\n  return res.slice(0, 150);\n}\n```\n\n### AI Reranking\n\nAfter merging the results, we use another LLM to rerank them based on their relevance to the original query.\n\nThe initial search and fusion steps are based on broader relevance signals. The reranker performs a more focused assessment of whether each result directly answers the user\'s question. It uses [baai/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base), a reranker model. These models are language models that reorder search results based on relevance to the user query, improving the qualify of the RAG. They take as input the user query and a list of documents, and return the order of the documents from most relevant to the query to least.\n\n```typescript title="src/search.ts"\nasync function processSearchResults(env: SearchBindings, params: DocSearchParams, mergedResults: { docId: string, id: string; score: number; text?: string }[]) {\n  if (!mergedResults.length) return { chunks: [] };\n  const { query, scoreThreshold, topK } = params;\n  const chunks: Array<{ text: string; id: string; docId: string } & { score: number }> = [];\n\n  const response = await env.AI.run(\n    "@cf/baai/bge-reranker-base",\n    {\n      // @ts-ignore\n      query,\n      contexts: mergedResults.map(r => ({ id: r.id, text: r.text }))\n    },\n  ) as { response: Array<{ id: number, score: number }> };\n\n  const scores = response.response.map(i => i.score);\n  let indices = response.response.map((i, index) => ({ id: i.id, score: sigmoid(scores[index]) }));\n  if (scoreThreshold && scoreThreshold > 0) {\n    indices = indices.filter(i => i.score >= scoreThreshold);\n  }\n  if (topK && topK > 0) {\n    indices = indices.slice(0, topK)\n  }\n\n  const slice = reorderArray(mergedResults, indices.map(i => i.id)).map((v, index) => ({ ...v, score: indices[index]?.score }));\n\n  await Promise.all(slice.map(async result => {\n    if (!result) return;\n    const a = {\n      text: result.text || (await getChunk(env, { docId: result.docId, id: result.id }))?.text || "",\n      docId: result.docId,\n      id: result.id,\n      score: result.score,\n    };\n\n    chunks.push(a)\n  }));\n\n  return { chunks };\n}\n```\n\nAfter reranking, we apply a sigmoid function to transform the raw scores:\n\n```typescript title="src/search.ts"\nfunction sigmoid(score: number, k: number = 0.4): number {\n  return 1 / (1 + Math.exp(-score / k));\n}\n```\n\nThe reranker\'s raw scores can have a wide range and don\'t directly translate to a probability of relevance. The sigmoid function has a few compelling characteristics:\n\n1. **Bounded output range**: Sigmoid squashes values into a fixed range (0,1), creating a probability-like score that\'s easier to interpret and threshold.\n2. **Non-linear transformation**: Sigmoid emphasizes differences in the middle range while compressing extremes, which is ideal for relevance scoring where we need to distinguish between "somewhat relevant" and "very relevant" items.\n3. **Stability with outliers**: Extreme scores don\'t disproportionately affect the normalized range.\n4. **Ordering**: Sigmoid maintains the relative ordering of results\n\nThe parameter `k` controls the steepness of the sigmoid curve: a smaller value creates a sharper distinction between relevant and irrelevant results. The specific value of `k=0.4` was chosen after experimentation to create an effective decision boundary around the 0.5 mark. Results above this threshold are considered sufficiently relevant for inclusion in the final context.\n\nBelow is a plot of our sigmoid function:\n\nWe pass the reranker score of each chunk through the `signoid` function, and we can set a fixed threshold score as the `sigmoid` normalises the scores. We can ignore anything below the threshold score as these chunks are likely not relevant to the user query.\n\nWithout the `sigmoid` function, each ranking would have a different score distribution and it would be difficult to fairly select a threshold score.\n\n### Search Parameter Tuning\n\nThe `searchDocs` function includes several parameters that significantly impact retrieval quality:\n\n```typescript title="src/index.ts"\nconst { chunks } = await searchDocs(c.env, {\n  questions: queries,\n  query: prompt,\n  keywords,\n  topK: 8,\n  scoreThreshold: 0.501,\n  ...searchOptions,\n});\n```\n\nThe `topK` parameter limits the number of chunks we retrieve, which is essential for:\n\n1. Managing the LLM context window size limitations\n2. Reducing noise in the context that could distract the model\n3. Minimizing cost and latency\n\nThe `scoreThreshold` is calibrated to work with our `sigmoid` normalization. Since sigmoid transforms scores to the (0,1) range with 0.5 representing the inflection point, setting the threshold just above 0.5 ensures we only include chunks that the reranker determined are more likely relevant than not.\n\nThis threshold prevents the inclusion of marginally relevant content that might dilute the quality of our context window.\n\n### Answer Generation\n\nFinally, we generate an answer using the retrieved chunks as context:\n\n```typescript\nconst answer = await c.env.AI.run("@cf/meta/llama-3.3-70b-instruct-fp8-fast", {\n  prompt: `${prompt}\n  \nContext: ${chunks}`\n})\n```\n\nThis step is where the "Generation" part of RAG comes in. The LLM receives both the user\'s question and the retrieved context, then generates an answer that draws on that context.\n\n### Stitching it all together\n\nThe entire system is an integrated pipeline where each component builds upon the previous ones:\n\n1. When a query arrives, it first hits the Query Rewriter, which expands it into multiple variations to improve search coverage.\n\n2. These expanded queries alongside extracted keywords simultaneously feed into two separate search systems:\n   - Vector Search (semantic similarity using embeddings)\n   - Full-Text Search (keyword matching using FTS5)\n\n3. The results from both search methods then enter the Reciprocal Rank Fusion function, which combines them based on rank position. This solves the challenge of comparing scores from fundamentally different systems.\n\n4. The fused results are then passed to the AI Reranker, which performs an LLM based relevance assessment focused on answering the original query.\n\n5. The reranked results are then filtered by score threshold and count limits before being passed into the context window for the final LLM.\n\n6. The LLM receives both the original query and the curated context to produce the final answer.\n\nThis multi-stage approach creates a system that outperforms traditional RAG systems.\n\n## Testing It Out\n\nNow let\'s see how our contextual RAG system works in practice. After deploying your Worker using `wrangler`, you\'ll get a URL for your Worker:\n\n```txt\nhttps://contextual-rag.account-name.workers.dev\n```\n\nwhere `account-name` is the name of your Cloudflare account.\n\nHere are some example API calls:\n\n#### Uploading a document\n\n```bash\ncurl -X POST https://contextual-rag.account-name.workers.dev/ \\\n  -H "Content-Type: application/json" \\\n  -d \'{"contents":"Paris doesn’t shout. It smirks. It moves with a kind of practiced nonchalance, a shrug that says, of course it’s beautiful here. It’s a city built for daydreams and contradictions — grand boulevards designed for kings now flooded with delivery bikes and tourists holding maps upside-down. Crumbling stone facades wear ivy and grime like couture. Every corner feels staged, but somehow still effortless, like the city isn’t even trying to impress you. It just is. Each arrondissement spins its own little universe. In the Marais, tiny alleys drip with charm — cafés packed so tightly you can hear every clink of every espresso cup. Montmartre clings to its hill like a stubborn old cat... <REDACTED>"}\'\n\n# Response:\n# {\n#   "id": "jtmnofvveptdalwl",\n#   "contents": "Paris doesn’t shout. It smirks. It moves with a kind of...",\n#   "created": "2025-04-26T19:50:39.941Z",\n#\t  "updated": "2025-04-26T19:50:39.941Z",\n# }\n```\n\n#### Querying\n\n```bash\ncurl -X POST https://contextual-rag.account-name.workers.dev/query \\\n  -H "Content-Type: application/json" \\\n  -d \'{"prompt":"What is special about Paris?"}\'\n\n# Response:\n# {\n#   "keywords": [\n#   \t"paris",\n#   \t"eiffel tower",\n#   \t"monet",\n#   \t"loire river",\n#   \t"montmartre",\n#   \t"notre dame",\n#   \t"art",\n#   \t"history",\n#   \t"culture",\n#   \t"tourism"\n#   ],\n#   "queries": [\n#   \t"paris attractions",\n#   \t"paris landmarks",\n#   \t"paris history",\n#   \t"paris culture",\n#   \t"paris tourism"\n#   ],\n#   "chunks": [\n#     {\n#       "text": "This chunk describes Paris, the capital of France, its location on the Seine River, and its famous landmarks and characteristics.; Paris doesn’t shout. It smirks. It moves with a kind of practiced nonchalance, a shrug that says, of course it’s beautiful here. It’s a city built for daydreams and contradictions...",\n#       "id": "abcdefghijklmno",\n#       "docId": "qwertyuiopasdfg",\n#       "score": 0.8085310556071287\n#     }\n#   ],\n#   "answer": "Paris, the capital of France, is known as the City of Light. It has been a hub for intellectuals and artists for centuries, and its stunning architecture, art museums, and romantic atmosphere make it one of the most popular tourist destinations in the world. Some of the most famous landmarks in Paris include the Eiffel Tower, the Louvre Museum...",\n#   "docs": [\n#     {\n#       "id": "jtmnofvveptdalwl",\n#       "contents": "Paris doesn’t shout. It smirks. It moves with a kind of practiced nonchalance, a shrug that says, of course it’s beautiful here. It’s a city built for daydreams and contradictions...",\n#       "created": "2025-04-26T19:50:39.941Z",\n#\t\t\t  "updated": "2025-04-26T19:50:39.941Z",\n#       "score": 0.8085310556071287\n#     }\n#   ]\n# }\n```\n\n## Limitations and Considerations\n\nAs much as this contextual RAG system is powerful, there are several limitations and considerations to keep in mind:\n\n1. **AI Costs**: The contextual enhancement process requires running each chunk through an LLM, which increases both the computation time and cost compared to traditional RAG systems. For very large document collections, this can become a significant consideration.\n\n2. **Latency Trade-offs**: Adding multiple stages of processing (query rewriting, hybrid search, reranking) improves result quality but increases end-to-end latency. For applications where response time is critical, you might need to optimize certain components or make trade-offs.\n\n3. **Storage Growth**: The contextualized chunks are longer than raw chunks, requiring more storage in both D1 and Vectorize. It\'s important to monitor the size of your databases and remain below limits.\n\n4. **Rate Limits**: All components of the pipeline have rate-limits. This can affect high-throughput applications. Consider offloading the document ingestion into a queue in production.\n\n5. **Context Window Limitations**: When contextualizing chunks, very long documents may exceed the context window of the LLM. You might need to implement a hierarchical approach for extremely large documents.\n\nOn [Basebrain](https://basebrain.ai), I\'ve addressed some of these challenges by using [Cloudflare Workflows](https://developers.cloudflare.com/workflows/) for document ingestion, and Durable Objects instead of D1 for storage, where I implement [one database per user](/blog/durable-objects-database-per-user.mdx).',
    },
    {
        "title": "One Database Per User with Cloudflare Durable Objects and Drizzle ORM",
        "link": "https://boristane.com/blog/durable-objects-database-per-user/",
        "guid": "https://boristane.com/blog/durable-objects-database-per-user/",
        "description": "Using Durable Objects and Drizzle ORM to create isolated SQLite databases for each user in a Hono app",
        "pubDate": "Sun, 23 Mar 2025 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": 'I\'m building a production-ready app entirely on the [Cloudflare Developers Platform](https://workers.dev) ([Basebrain](https://basebrain.ai)) and I want to share implementation details of the key patterns I use.\n\nToday, I\'m focusing on a specific pattern that\'s surprisingly simple to implement: creating one isolated database per user using Durable Objects and Drizzle ORM.\n\n[Durable Objects](https://www.cloudflare.com/developer-platform/products/durable-objects/) are one of the most powerful primitives on the Cloudflare Developer Platform, but I have seen many people struggle to define them. I see them as **mini servers without the ops**.\n\nEach object (server) is:\n\n- Addressable by a unique name/ID, like a key in a distributed system.\n- Run in a single location at any given time, so no race conditions.\n- Can store persistent state, backed by Durable Object Storage.\n- Can coordinate between multiple clients, like a chat room, game session, or rate limiter.\n\nThey\'re great when you need strong consistency, low-latency coordination, or lightweight stateful logic.\n\nDurable Objects provide a convenient [SQLite API](https://blog.cloudflare.com/sqlite-in-durable-objects/), and when combined with Drizzle ORM, give you a clean, type-safe way to create a database per user in your applications.\n\n## One Database Per User\n\nBefore looking at implementation, let\'s understand why you might want one database per user:\n\n1. **Scalability** - Sharding a database into one instance per user addresses the limitations of storing and managing large datasets on a single database server\n2. **True isolation** - Each user\'s data is completely isolated\n3. **Simplified access control** - No complex filtering or multi-tenancy code\n4. **Performance** - Durable Objects are distributed globally, and instanciated closest to the first request they receive. As such, each database will be instanciated closest to its users, significantly reducing end-to-end latency\n\nIn this example, we\'ll build a simple notes API where each user gets their own SQLite database. A user can create, read, list, or delete notes. They will be able to interact exclusively with their own database. By definition, a user will not be able to access another user\'s data.\n\nThe complete code for this tutorial is available on [GitHub](https://github.com/boristane/cloudflare-dev-101/tree/main/durable-objects-database-per-user).\n\nThroughout this example, we\'ll assume you\'re familiar with deploying [Cloudflare Workers](https://developers.cloudflare.com/workers/) and we will focus on Durable Objects.\n\n## Project Overview\n\nLet\'s start with an overview of the project. We\'ll be using:\n\n- [**Cloudflare Workers**](https://developers.cloudflare.com/workers/) - Our serverless platform\n- [**Durable Objects**](https://developers.cloudflare.com/durable-objects/) - To maintain per-user databases\n- [**Drizzle ORM**](https://orm.drizzle.team/) - For type-safe database operations\n- [**Hono**](https://hono.dev/) - A lightweight framework for our API routes\n\nHere\'s the basic structure of the project:\n\n```\n.\n├── bindings.ts                # Bindings TypeScript Definition\n├── package.json               # Dependencies and scripts\n├── src\n│   ├── db                     # Database-related code\n│   │   ├── index.ts           # CRUD operations\n│   │   ├── notes.ts           # Schema definition\n│   │   ├── schemas.ts         # Schema exports\n│   │   └── types.ts           # TypeScript types\n│   └── index.ts               # Main Worker code and Durable Object\n├── tsconfig.json              # TypeScript configuration\n└── wrangler.json              # Cloudflare Workers configuration\n```\n\n## Bootstrapping the Durable Object\n\nLet\'s start with bootstrapping our Durable Object code.\n\nEach Durable Object is part of a [Durable Object Namespace](https://developers.cloudflare.com/durable-objects/api/namespace/). A Durable Object Namespace is how you reference a Durable Object class from your Worker. It\'s like a binding that lets you create or execute methods in the Durable Objects from your Worker. A Durable Object Namespace must be bound to your Worker before you can instanciate Durable Objects.\n\nAfter creating a Node.js project with `npm` and configuring TypeScript, let\'s create our `wrangler.json` file to configure our Worker and Durable Object Namespace.\n\n```json title="wrangler.json"\n{\n  "name": "durable-objects-database-per-user",\n  "compatibility_date": "2024-11-12",\n  "workers_dev": true,\n  "upload_source_maps": true,\n  "observability": {\n    "enabled": true\n  },\n  "main": "./src/index.ts",\n  "migrations": [\n    {\n      "new_sqlite_classes": [\n        "DurableDatabase"\n      ],\n      "tag": "v1"\n    }\n  ],\n  "durable_objects": {\n    "bindings": [\n      {\n        "class_name": "DurableDatabase",\n        "name": "DurableDatabase"\n      }\n    ]\n  }\n}\n\n```\n\nOur Worker script name will be `durable-objects-database-per-user`, it will have a `worker.dev` URL, and observability enabled. Our Durable Object class will be `DurableDatabase` and we enabled the Durable Object SQLite API on it. \n\nNext, let\'s write the code for our Worker and our Durable Objects.\n\n```ts title="src/index.ts"\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\napp.get(\'/\', async (c) => {\n  return c.json({ message: "Hello World!" })\n});\n\nexport default {\n  async fetch(request: Request, env: Bindings, ctx: ExecutionContext): Promise<Response> {\n    return app.fetch(request, env, ctx);\n  },\n};\n\nexport class DurableDatabase extends DurableObject {\n  constructor(ctx: DurableObjectState, env: Bindings) {\n    super(ctx, env);\n  }\n}\n```\n\nIt\'s a Hono API, and the `DurableDatabase` doesn\'t do anything yet. We will come back to implement its methods.\n\nWe import `Bindings` from our bindings definition file. The `Bindings` type enables us to have type-safety and auto-completion when interacting with resources bound to the Worker.\n\n```typescript title="src/bindings.ts"\n\nexport type Bindings = {\n  DurableDatabase: DurableObjectNamespace<DurableDatabase>;\n};\n```\nWe are pointing out to TypeScript that there\'s a Durable Object Namespace called `DurableDatabase` implementing the Durable Object class `DurableDatabase` bound to our Worker.\n\n## Database Schema\n\nNow let\'s define our database schema. Let\'s keep it simple: a note has an ID, a text content, and created and updated timestamps.\n\nWe use the `sqliteTable` function from Drizzle ORM to create schema for our table:\n\n```ts title="src/db/notes.ts"\n\nexport const notes = sqliteTable(\n  "notes",\n  {\n    id: text("id")\n      .notNull()\n      .primaryKey()\n      .$defaultFn(() => `note_${randomString()}`),\n    \n    text: text("text").notNull(),\n    \n    created: integer("created", { mode: "timestamp_ms" })\n      .$defaultFn(() => new Date())\n      .notNull(),\n    updated: integer("updated", { mode: "timestamp_ms" })\n      .$onUpdate(() => new Date())\n      .notNull(),\n  },\n);\n\n// Helper to generate random IDs for our notes\nfunction randomString(length = 16): string {\n  const chars = "abcdefghijklmnopqrstuvwxyz";\n  const resultArray = new Array(length);\n\n  for (let i = 0; i < length; i++) {\n    const randomIndex = Math.floor(Math.random() * chars.length);\n    resultArray[i] = chars[randomIndex];\n  }\n\n  return resultArray.join("");\n}\n```\n\nAlongside our schemas, we need to define the TypeScript types of notes saved in the database. We can infer those from the schema thanks to Drizzle `$infer` properties.\n\n```ts title="src/db/types.ts"\n\nexport type Note = typeof notes.$inferSelect;\nexport type InsertNote = typeof notes.$inferInsert;\n```\n\nWe also create a type for our Database, such that we can easily use it when making database operations.\n\n```ts title="src/db/types.ts" ins={1,5}\n\nexport type DB = DrizzleSqliteDODatabase<typeof schema>;\n\nexport type Note = typeof notes.$inferSelect;\nexport type InsertNote = typeof notes.$inferInsert;\n```\n\nAnd we export all the schemas in a single file.\n\n```ts title="src/db/schemas.ts"\nexport * from "./notes";\n```\n\n## Database Operations\n\nNow that we have a schema, let\'s create the functions that will interact with our database. These will be the core operations our Durable Object will expose. Each use should have the ability to create a new note, delete a note by ID, get a specific note by ID, and list all notes.\n\n```typescript title="src/db/index.ts"\n\n// Create a new note\nexport async function create(db: DB, note: InsertNote): Promise<Note> {\n  const [res] = await db\n    .insert(notes)\n    .values(note)\n    .onConflictDoUpdate({\n      target: [notes.id],\n      set: note,\n    })\n    .returning();\n\n  return res;\n}\n\n// Delete a note by ID\nexport async function del(db: DB, params: { id: string }): Promise<Note> {\n  const [note] = await db\n    .delete(notes)\n    .where(eq(notes.id, params.id))\n    .returning();\n  return note;\n}\n\n// Get a note by ID\nexport async function get(db: DB, params: { id: string }): Promise<Note | null> {\n  const [result] = await db\n    .select()\n    .from(notes)\n    .where(eq(notes.id, params.id));\n  if (!result) return null;\n  return result;\n}\n\n// List all notes\nexport async function list(db: DB): Promise<Note[]> {\n  const ns = await db\n    .select()\n    .from(notes)\n  return ns;\n}\n```\n\nThese operations are using fairly standard Drizzle\'s query builder API functions, it gives us a clean, fluent interface for building SQL queries.\n\n## Implementing the Durable Object\n\nNow let\'s look at how we implement our Durable Object. This is where the magic happens, each Durable Object instance will be tied to a specific user and contain their own SQLite database.\n\n```typescript title="src/index.ts" ins={4-6,8,27-28,30-45}\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\napp.get(\'/\', async (c) => {\n  return c.json({ message: "Hello World!" })\n});\n\nexport default {\n  async fetch(request: Request, env: Bindings, ctx: ExecutionContext): Promise<Response> {\n    return app.fetch(request, env, ctx);\n  },\n};\n\nexport class DurableDatabase extends DurableObject {\n  private db: DB;\n  \n  constructor(ctx: DurableObjectState, env: Bindings) {\n    super(ctx, env);\n    // Initialize Drizzle with the Durable Object\'s storage\n    this.db = drizzle(ctx.storage, { schema, logger: true });\n  }\n\n  async notesCreate(note: Parameters<typeof notes.create>[1]): ReturnType<typeof notes.create> {\n    return await notes.create(this.db, note);\n  }\n\n  async notesGet(params: Parameters<typeof notes.get>[1]): ReturnType<typeof notes.get> {\n    return await notes.get(this.db, params);\n  }\n\n  async notesList(): ReturnType<typeof notes.list> {\n    return await notes.list(this.db);\n  }\n\n  async notesDel(params: Parameters<typeof notes.get>[1]): ReturnType<typeof notes.del> {\n    return await notes.del(this.db, params);\n  }\n}\n```\n\nLet\'s break down what\'s happening here:\n\n1. In the constructor, we initialize Drizzle with `ctx.storage`, which is the Durable Object\'s built-in storage that SQLite operates on. This gives us a `DrizzleSqliteDODatabase` which is unique per Durable Object. Everytime we do an operation using this object, we are interacting with a single SQLite instance.\n\n3. We create methods that map directly to our database operations. These methods will be called from our API routes.\n\nThe key insight here is that each instance of our `DurableDatabase` class is completely isolated from other instances. When we create a Durable Object for a user, they get their own private database.\n\n## Database Migrations\n\nDatabase migration for Durable Object SQLite can be confusing, but the key thing to understand is that migrations must be run in the constructor of the Durable Object class.\n\nDurable Object constructors run when a request is routed to an instance that isn\'t currently active, basically on cold start. When you try to access a specific Durable Object from a Worker using its name/ID, if the Durable Object for that ID isn\'t already in memory somewhere in Cloudflare\'s edge network, a new instance spins up and the constructor is run once.\n\nAfter that, the object stays warm for a while (usually minutes of inactivity) before being evicted from memory. If another request comes in later, the object will spin up again and the constructor is called again.\n\nIt then makes sense to run migrations in the Durable Object class constructor. When a new request is routed to a specific Durable Objects, the migration files since the last migration will be run in the constructor. This enables doing migrations on potentially millions of Durable Objects, "just-in-time".\n\n```typescript title="src/index.ts" ins={5-6,31-35,53-56}\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\napp.get(\'/\', async (c) => {\n  return c.json({ message: "Hello World!" })\n});\n\nexport default {\n  async fetch(request: Request, env: Bindings, ctx: ExecutionContext): Promise<Response> {\n    return app.fetch(request, env, ctx);\n  },\n};\n\nexport class DurableDatabase extends DurableObject {\n  private db: DB;\n  \n  constructor(ctx: DurableObjectState, env: Bindings) {\n    super(ctx, env);\n    // Initialize Drizzle with the Durable Object\'s storage\n    this.db = drizzle(ctx.storage, { schema, logger: true });\n    \n    // Run migrations before accepting any requests\n    ctx.blockConcurrencyWhile(async () => {\n      await this._migrate();\n    });\n  }\n\n  async notesCreate(note: Parameters<typeof notes.create>[1]): ReturnType<typeof notes.create> {\n    return await notes.create(this.db, note);\n  }\n\n  async notesGet(params: Parameters<typeof notes.get>[1]): ReturnType<typeof notes.get> {\n    return await notes.get(this.db, params);\n  }\n\n  async notesList(): ReturnType<typeof notes.list> {\n    return await notes.list(this.db);\n  }\n\n  async notesDel(params: Parameters<typeof notes.get>[1]): ReturnType<typeof notes.del> {\n    return await notes.del(this.db, params);\n  }\n\n  private async _migrate() {\n    await migrate(this.db, migrations);\n  }\n}\n```\n\nWe use `blockConcurrencyWhile` to run migrations before accepting any requests. This ensures our database schema is up-to-date.\n\nYou\'re asking yourself what is the `migrations` we import from `../drizzle/migrations`. These are database migration files created by [Drizzle Kit](https://orm.drizzle.team/docs/kit-overview). Drizzle Kit is a CLI tool for managing SQL database migrations with Drizzle.\n\nLet\'s define our Drizzle config with the `durable-sqlite` driver.\n\n```ts title="drizzle.config.ts"\n\nexport default defineConfig({\n  out: \'./drizzle\',\n  schema: \'./src/db/schemas.ts\',\n  dialect: \'sqlite\',\n  driver: \'durable-sqlite\',\n});\n```\n\nTo create the migration files, it\'s necessary to run the Drizzle Kit `generate` command:\n\n```bash\ndrizzle-kit generate\n```\n\nThis command will create a new folder `drizzle` in our project structure, which will keep track of all our migrations. After every change to our database schema, we need to run the same command to generate migration files.\n\nThese migration files need to be uploaded alongside our Worker code, such that they can be run on Durable Object cold-starts. However, [`wrangler`](https://developers.cloudflare.com/workers/wrangler/), the Cloudflare Developer Platform command-line interface, only uploads files it generates when bundling a project. It is necessary to explicitely require migration files to be uploaded alongside the Worker code.\n\n```json title="wrangler.json" ins={18-26}\n{\n  "name": "durable-objects-database-per-user",\n  "compatibility_date": "2024-11-12",\n  "workers_dev": true,\n  "upload_source_maps": true,\n  "observability": {\n    "enabled": true\n  },\n  "main": "./src/index.ts",\n  "migrations": [\n    {\n      "new_sqlite_classes": [\n        "DurableDatabase"\n      ],\n      "tag": "v1"\n    }\n  ],\n  "rules": [\n    {\n      "type": "Text",\n      "globs": [\n        "**/*.sql"\n      ],\n      "fallthrough": true\n    }\n  ],\n  "durable_objects": {\n    "bindings": [\n      {\n        "class_name": "DurableDatabase",\n        "name": "DurableDatabase"\n      }\n    ]\n  }\n}\n```\n\nThe `rules` section tells Wrangler to include all `.sql` files in our deployment. Without this rule, our SQL migration files wouldn\'t be included in the deployment, and our migrations would fail.\n\n## Setting Up API Routes\n\nNow let\'s set up our API routes to interact with our Durable Objects.\n\n```typescript title="src/index.ts" ins={23-65}\n\nconst app = new Hono<{ Bindings: Bindings }>();\n\napp.get(\'/\', async (c) => {\n  return c.json({ message: "Hello World!" })\n});\n\nexport default {\n  async fetch(request: Request, env: Bindings, ctx: ExecutionContext): Promise<Response> {\n    return app.fetch(request, env, ctx);\n  },\n};\n\nfunction getDurableDatabaseStub(env: Bindings, userId: string) {\n  const doId = env.DurableDatabase.idFromName(userId);\n  return env.DurableDatabase.get(doId);\n} \n\n// Create a note for a user\napp.post(\'/:userId\', async (c) => {\n  const userId = c.req.param("userId");\n  const { text } = await c.req.json();\n  const stub = getDurableDatabaseStub(c.env, userId);\n  const note = await stub.notesCreate({ text });\n  return c.json({ note })\n});\n\n// List all notes for a user\napp.get(\'/:userId\', async (c) => {\n  const userId = c.req.param("userId");\n  const stub = getDurableDatabaseStub(c.env, userId);\n  const notes = await stub.notesList()\n  return c.json({ notes })\n});\n\n// Get a specific note for a user\napp.get(\'/:userId/:noteId\', async (c) => {\n  const userId = c.req.param("userId");\n  const noteId = c.req.param("noteId");\n  const stub = getDurableDatabaseStub(c.env, userId);\n  const note = await stub.notesGet({ id: noteId });\n  if (!note) {\n    return c.notFound();\n  }\n  return c.json({ note })\n});\n\n// Delete a note for a user\napp.delete(\'/:userId/:noteId\', async (c) => {\n  const userId = c.req.param("userId");\n  const noteId = c.req.param("noteId");\n  const stub = getDurableDatabaseStub(c.env, userId);\n  const note = await stub.notesDel({ id: noteId });\n  return c.json({ note })\n});\n\nexport class DurableDatabase extends DurableObject {\n  private db: DB;\n  \n  constructor(ctx: DurableObjectState, env: Bindings) {\n    super(ctx, env);\n    // Initialize Drizzle with the Durable Object\'s storage\n    this.db = drizzle(ctx.storage, { schema, logger: true });\n    \n    // Run migrations before accepting any requests\n    ctx.blockConcurrencyWhile(async () => {\n      await this._migrate();\n    });\n  }\n\n  async notesCreate(note: Parameters<typeof notes.create>[1]): ReturnType<typeof notes.create> {\n    return await notes.create(this.db, note);\n  }\n\n  async notesGet(params: Parameters<typeof notes.get>[1]): ReturnType<typeof notes.get> {\n    return await notes.get(this.db, params);\n  }\n\n  async notesList(): ReturnType<typeof notes.list> {\n    return await notes.list(this.db);\n  }\n\n  async notesDel(params: Parameters<typeof notes.get>[1]): ReturnType<typeof notes.del> {\n    return await notes.del(this.db, params);\n  }\n\n  private async _migrate() {\n    await migrate(this.db, migrations);\n  }\n}\n```\n\nThe key pattern in each route is:\n\n- Extract the `userId` from the URL\n- Get a Durable Object ID using `idFromName(userId)`\n- Get a stub to the Durable Object using `get(doId)`\n- Call methods on the stub to interact with the user\'s database\n\nA durable object stub is the proxy we use to send requests to a specific Durable Object instance. We get the Durable Object ID from the `userId`, and we use it to instanciate a lightweight proxy object (the stub).\n\n```ts\nfunction getDurableDatabaseStub(env: Bindings, userId: string) {\n  const doId = env.DurableDatabase.idFromName(userId);\n  return env.DurableDatabase.get(doId);\n} \n```\n\nWith the stub, we can call methods on the Durable Object using [JavaScript-native RPC](https://blog.cloudflare.com/javascript-native-rpc/). That\'s how we\'re able to call the `notesCreate`, `notesGet`, `notesDel`, and `notesList` methods in the Durable Object.\n\nThis is the core of our "one database per user" pattern. By using `idFromName(userId)`, we\'re creating a consistent mapping from user IDs to Durable Objects. Each user gets their own Durable Object, and thus their own database.\n\n## Testing It Out\n\nNow let\'s see how our "one database per user" pattern works in practice. After you have deployed your Worker using `wrangler`, you\'ll get a URL for your Worker:\n\n```txt\nhttps://durable-objects-database-per-user.account-name.workers.dev\n```\n\n where `account-name` is the name of your account.\n\nHere are some example API calls:\n\n#### Creating a note for user "john"\n\n```bash\ncurl -X POST https://durable-objects-database-per-user.account-name.workers.dev/john \\\n  -H "Content-Type: application/json" \\\n  -d \'{"text":"Buy groceries"}\'\n\n# Response:\n# {\n#   "note": {\n#     "id": "note_jxqegmbonzvdstpy",\n#     "text": "Buy groceries",\n#     "created": "2025-03-23T15:42:18.760Z",\n#     "updated": "2025-03-23T15:42:18.760Z"\n#   }\n# }\n```\n\n#### Listing all notes for "john"\n\n```bash\ncurl https://durable-objects-database-per-user.account-name.workers.dev/john\n\n# Response:\n# {\n#   "notes": [\n#     {\n#       "id": "note_jxqegmbonzvdstpy",\n#       "text": "Buy groceries",\n#       "created": "2025-03-23T15:42:18.760Z",\n#       "updated": "2025-03-23T15:42:18.760Z"\n#     }\n#   ]\n# }\n```\n\n#### Creating a note for user "bob"\n\n```bash\ncurl -X POST https://durable-objects-database-per-user.account-name.workers.dev/bob \\\n  -H "Content-Type: application/json" \\\n  -d \'{"text":"Finish the report"}\'\n\n# Response:\n# {\n#   "note": {\n#     "id": "note_lnkcpfqyrtzbmags",\n#     "text": "Finish the report",\n#     "created": "2025-03-23T15:43:05.120Z",\n#     "updated": "2025-03-23T15:43:05.120Z"\n#   }\n# }\n```\n\n#### Listing all notes for "bob"\n\n```bash\ncurl https://durable-objects-database-per-user.account-name.workers.dev/bob\n\n# Response:\n# {\n#   "notes": [\n#     {\n#       "id": "note_lnkcpfqyrtzbmags",\n#       "text": "Finish the report",\n#       "created": "2025-03-23T15:43:05.120Z",\n#       "updated": "2025-03-23T15:43:05.120Z"\n#     }\n#   ]\n# }\n```\n\nThe key insight here is that "john" and "bob" have completely separate databases.\n\n## Limitations and Considerations\n\nWhile this pattern is powerful, there are a few things to keep in mind:\n\n1. **Storage Limits** - Each Durable Object has a storage limit (currently 10GB), which could be limiting and push us to further sharding depending on the scale of our app\n2. **Cross-User Operations** - Queries across multiple users\' data, for example back-office queries or analytics, require additional work\n\nOn [Basebrain](https://basebrain.ai), I\'m going around these with further sharding and by using a different database for analytics, [Workers Analytics Engine](https://blog.cloudflare.com/workers-analytics-engine/).',
    },
    {
        "title": "Observing Serverless Applications",
        "link": "https://boristane.com/talks/observing-serverless-applications/",
        "guid": "https://boristane.com/talks/observing-serverless-applications/",
        "description": "Best practices for observing serverless applications",
        "pubDate": "Tue, 26 Nov 2024 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "I teamed up with [Kyle](https://www.linkedin.com/in/kylegalbraith459/) to share the best practices to observe serverless applications.\n\nThe talk is all about high-cardinality and dimensionality wide events, how to create them and leverage to build more observable serverless applications, with practical examples and real-world scenarios.",
    },
    {
        "title": "Observability wide events 101",
        "link": "https://boristane.com/blog/observability-wide-events-101/",
        "guid": "https://boristane.com/blog/observability-wide-events-101/",
        "description": "What are wide events, why and how should you implement them?",
        "pubDate": "Sat, 07 Sep 2024 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": 'Wide events are a very simple concept: for each request, emit a single context-rich event/log per service hop. That\'s it. Don\'t let all the buzzwords fool you.\n\nLet\'s say you\'re building a blogging platform, and users can save articles. A simplified implementation of the POST /articles endpoint might look like this:\n\n- User makes a request from a browser\n- Request hits your `gateway` which authenticates and orchestrates various downstream services\n- `gateway` calls the `articles` service. This service adds the article in the database and caches the result\n- `gateway` calls the `notifications` service which emails all relevant subscribers to the blog\n- `gateway` calls the `analytics` service which sends a message to a queue for product analytics \n\nIn a nutshell, each of these services, from the browser to the database and queue, should emit a single structured wide-event with an arbitrary number of fields. All these events must be connected with a request ID, such that you can correlate all of them together.\n\n## Why use wide events? \n\nZooming in on the `articles` service, its events should include details from your business logic such as the user, their subscription, the saved article, the response status code, etc.:\n\n```json showLineNumbers=false\n{\n  "method": "POST",\n  "path": "/articles",\n  "service": "articles",\n  "outcome": "ok",\n  "status_code": 201,\n  "duration": 268,\n  "requestId": "8bfdf7ecdd485694",\n  "timestamp":"2024-09-08 06:14:05.680",\n  "message": "Article created",\n  "commit_hash": "690de31f245eb4f2160643e0dbb5304179a1cdd3",\n  "user": {\n    "id": "fdc4ddd4-8b30-4ee9-83aa-abd2e59e9603",\n    "activated": true,\n    "subscription": {\n      "id": "1aeb233c-1572-4f54-bd10-837c7d34b2d3",\n      "trial": true,\n      "plan": "free",\n      "expiration": "2024-09-16 14:16:37.980",\n      "created": "2024-08-16 14:16:37.980",\n      "updated": "2024-08-16 14:16:37.980"\n    },\n    "created": "2024-08-16 14:16:37.980",\n    "updated": "2024-08-16 14:16:37.980"\n  },\n  "article": {\n    "id": "f8d4d21c-f1fd-48b9-a4ce-285c263170cc",\n    "title": "Test Blog Post",\n    "ownerId": "fdc4ddd4-8b30-4ee9-83aa-abd2e59e9603",\n    "published": false,\n    "created": "2024-09-08 06:14:05.460",\n    "updated": "2024-09-08 06:14:05.460"\n  },\n  "db": {\n    "query": "INSERT INTO articles (id, title, content, owner_id, published, created, updated) VALUES ($1, $2, $3, $4, $5, $6, $7);",\n    "parameters": {\n      "$1": "f8d4d21c-f1fd-48b9-a4ce-285c263170cc",\n      "$2": "Test Blog Post",\n      "$3": "******",\n      "$4": "fdc4ddd4-8b30-4ee9-83aa-abd2e59e9603",\n      "$5": false,\n      "$6": "2024-09-08 06:14:05.460",\n      "$7": "2024-09-08 06:14:05.460"\n    }\n  },\n  "cache": {\n    "operation": "write",\n    "key": "f8d4d21c-f1fd-48b9-a4ce-285c263170cc",\n    "value": "{\\"article\\":{\\"id\\":\\"f8d4d21c-f1fd-48b9-a4ce-285c263170cc\\",\\"title\\":\\"Test Blog Post\\"..."\n  },\n  "headers": {\n    "accept-encoding": "gzip, br",\n    "cf-connecting-ip": "*****",\n    "connection": "Keep-Alive",\n    "content-length": "1963",\n    "content-type": "application/json",\n    "host": "website.com",\n    "url": "https://website.com/articles",\n    "user-agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36",\n    "Authorization": "********",\n    "x-forwarded-proto": "https",\n    "x-real-ip": "******"\n  }\n}\n```\n\nAt a glance, you can see that an article was posted by a user on a free trial expiring on September 16th. The service responded with `201` status code in `268`ms.\n\nWide events must have the following characteristics:\n\n- **high cardinality**: each field can contain an unbounded number of unique values, such as user IDs, session IDs, or transaction IDs. You could have billions of these per day.\n- **high dimensionality**: by definition, wide events should have a large number of fields (dimensions) to provide deep insights\n- **context-rich**: all those fields should carry context about the request, from request headers to infrastructure details, and custom business logic data\n\n### The problem with logs and metrics\n\nWide events enable you to answer questions that are simply impossible to answer with traditional logs or metrics. Imagine, instead of wide events, we had opted for logs and metrics in the `articles` service:\n\n```txt showLineNumbers=false\n2024-09-08 06:14:05.280 Received POST /articles request\n2024-09-08 06:14:05.298 Saving article: f8d4d21c-f1fd-48b9-a4ce-285c263170cc\n2024-09-08 06:14:05.449 Article saved: f8d4d21c-f1fd-48b9-a4ce-285c263170cc\n2024-09-08 06:14:05.451 Response time: 254ms\n2024-09-08 06:14:05.460 Successful request: 201\n```\n\nAnd a set of metrics charts, for request duration, number of articles created and number of failures on the route.\n\nNow imagine this user emails you, saying every time they create an article, it doesn\'t appear on the website, along with a video showing the issue. And 67 more users give you the same feedback. How do you start debugging this. Your logs tell you everything is fine, the articles are in the database, and your metrics chart don\'t show any unexpected behaviour.\n\nThis is what we call **unknown unknowns**. Logs and metrics help capture \'known unknowns\' - issues you can anticipate while building your application. Things such as slow requests, errors, database failures, or obvious potential issues in your business logic; for example missing environment variables, or "impossible" code paths, etc.\n\nBut you are left hanging when it comes to unexpected behaviour you couldn\'t predict before your code got into the hands of real users and they started doing unexpected things.\n\nWith wide events, and the **appropriate tooling**, you can investigate and solve these issues without pulling your hair out. I emphasize on appropriate tooling; wide events are only the first piece, and without appropriate tooling you\'re only halfway there.\n\n### Tooling\n\nWhichever tool you use, it should have the following characteristics:\n{/* - **high cardinality**: each field in your event should support an arbitrary large number of unique values. For example request ID. A successful application could have 1000s of requests per second.\n- **high dimensionality**: each event should have an arbitrary large number of fields. From a handful to hundreads. */}\n- **queryable across any dimension**: you should be able to query across any of the fields in your events\n- **no pre-aggregation**: your events should be stored as they are emitted, without pre-aggregation. you should have access to the raw data, not just a value that was extracted from a batch of events\n- **fast**: querying your events should be fast, ideally sub-second; but definitely sub-minute.\n- **affordable**: observability should not bankrupt your application, sampling can drastically help here. \n\nI used to be a [vendor](https://baselime.io) and hitting all those points is extremely hard, but you should demand no less from your vendor or your custom-built solution. \n\n## How to use wide-events\n\nFor the eagle-eyed, the wide event we illustrated above actually has the answer to our previous bug (newly posted articles don\'t show on the website for a subset of users).\n\nBut let\'s plot investigate with some graphs based on our wide events. Let\'s start with number of articles posted. I\'ll use the SQL syntax to illustrate the queries I\'m writing to generate the graphs but your solution probably has a custom query language.\n\n### Number of articles posted\n\n```sql showLineNumbers=false\nselect count()\nfrom events\nwhere method = "POST"\nand path = "/articles"\nand status_code = 201\n```\n\nNothing to note on this chart, the number of articles successfully posted has been constant. But are they published? Let\'s group by `article.published`.\n\n### Number of articles posted grouped by `article.published`\n\n```sql showLineNumbers=false\nselect count()\nfrom events\nwhere method = "POST"\nand path = "/articles"\nand status_code = 201\ngroup by article.published\norder by count() desc\n```\n\nClearly, the reason users can\'t see their articles is because they are posted but they are set to `published = false`. But why? Let\'s see if it\'s just a subset of users or if it\'s a wide-spread issue.\n\n### Number of unpublished articles grouped by `user.id`\n\n```sql showLineNumbers=false\nselect count()\nfrom events\nwhere method = "POST"\nand path = "/articles"\nand status_code = 201\nand article.published = false\ngroup by user.id\norder by count() desc\n```\n\nOkay, this is not an isolated issue, multiple users are impacted. But how many exactly? Let\'s count all the unique users who posted an article and group them by `article.published`.\n\n### Number of unique users grouped by `article.published`\n\n```sql showLineNumbers=false\nselect count(unique user.id)\nfrom events\nwhere method = "POST"\nand path = "/articles"\nand status_code = 201\ngroup by article.published\n```\n\nVery few users were posting unpublished articles, and suddenly, most started posting unpublished articles. Clearly this is not a subset of users. But what do these users have in common? \n\nMore traditional tooling might get you up to here, if you have diligently implemented structured logs and you have a whole catalog of metrics and dashboards. The next step is usually to go almost randomly look at code, or rely on memory of the day code was written to find the issue.\n\nBut you don\'t need this with wide events. Your setup will tell you exactly what the issue is, you just have to ask.\n\nAre all these users with unpublished articles on the free trial?\n\n### Number of unique users grouped by `article.published` and `user.trial`\n\n```sql showLineNumbers=false\nselect count(unique user.id)\nfrom events\nwhere method = "POST"\nand path = "/articles"\nand status_code = 201\ngroup by article.published, user.trial\n```\n\nIt\'s obvious only users on the free trial are impacted. And given they out-number paid users, most articles are posted with `article.published = false`. It\'s clear paid customers are not impacted by the issue.\n\nIf additionally, your tooling enables you to write metadata markers such as commits or deployments, you can pinpoint the commit that introduced the defect.\n\nA single click and you\'re directly sent to the diff that caused the issue.\n\n## How to implement wide-events?\n\nHere\'s a very crude implementation of the basic principle of wide events.\n\n```ts\napp.post(\'/articles\', async (c) => {\n  const startTime = Date.now();\n\n  // initialise the wide event\n  const wideEvent: Record<string, unknown> = {\n    method: \'POST\',\n    path: \'/articles\',\n    service: \'articles\',\n    requestId: c.get("requestId"),\n    headers: c.req.raw.headers,\n    // optionally add then environment variables\n    // ensure no secrets are stored here\n    env: process.env,\n  };\n\n  try {\n    const body = await c.req.json();\n    const { title, content } = body;\n    const user = database.getUser(c.get("userId"));\n\n    wideEvent["user"] = user;\n\n    const article = {\n      id: uuidv4(),\n      title,\n      content,\n      ownerId: user.id,\n      published: true,\n    };\n\n    const { savedArticle, dbOperation } = await database.saveArticle(article);\n    wideEvent["article"] = savedArticle;\n    wideEvent["db"] = dbOperation;\n    \n    const cacheResponse = await cache.set(articleId, article);\n    wideEvent["cache"] = cacheResponse;\n\n    const response = { message: \'Article created\', article };\n    wideEvent["status_code"] = 201;\n    wideEvent["message"] = \'Article created\';\n    wideEvent["outcome"] = \'ok\';\n\n    return c.json(response, 201);\n  } catch (error) {\n    wideEvent["outcome"] = \'error\';\n    wideEvent["status_code"] = 500;\n    wideEvent["message"] = error.message;\n    return c.json({ error: \'Internal Error\' }, 500);\n  } finally {\n    const duration = Date.now() - startTime;\n    wideEvent["duration"] = duration;\n    wideEvent["timestamp"] = new Date().toISOString();\n\n    // flush the wide event\n    logger.info(JSON.stringify(wideEvent));\n  }\n});\n```\n\nYou can build from this, with middlewares, helper functions to add multiple keys simultaneously and also with queues to ensure the wide event is flushed even when the request fails or timesout.\n\n## What about OpenTelemetry?\n\nOur crude implementation of wide events above has a major flaw: how do you propagate the `requestId` we pick up at `Line 9` across services and multiple calls. [Distributed tracing](https://baselime.io/glossary/distributed-tracing) takes the idea of wide events and builds on top: It enables you to propagate `requestId` automatically, as well as capturing timestamps and keeping a hierarchy between multiple service calls. It also formalises the language around wide events.\n\nInstead of "wide event", within the context of distributed tracing, you will say [span](https://baselime.io/glossary/span).\n\nInstead of "request", you\'ll say [trace](https://baselime.io/glossary/trace). This is because distributed tracing also works outside the context of request/response applications. For background jobs, long running tasks and event streaming for example.\n\n[OpenTelemetry](https://opentelemetry.io/) is an attempt to further formalise the instrumentation, collection and exportation of distributed traces. It\'s a complex project with a very vast and rich history.\n\nIt\'s pretty easy today to get confused by Opentelemetry, but if you look at it as a simpler way to generate wide events, you\'re winning. You don\'t need to understand [Opentelemetry Collectors](https://opentelemetry.io/docs/collector/), [Baggage](https://opentelemetry.io/docs/concepts/signals/), or [Resources](https://opentelemetry.io/docs/concepts/resources/). Unless you\'re building your own OpenTelemetry backend, most of these concepts are mostly relevant to your vendor. \n\nPick the distro for your language / framework (a language distro is an SDK). Install and configure it to automatically capture all i/o calls, and figure out how to add custom attributes to the span (remember, a span is just a wide event).\n\nIn node.js it looks like\n\n```ts\napp.post(\'/articles\', async (c) => {\n  const currentSpan = trace.getSpan(context.active());\n\n  try {\n    const body = await c.req.json();\n    const { title, content, } = body;\n\n    const user = database.getUser(c.get("userId"));\n    currentSpan.setAttributes(user);\n\n    const article = {\n      id: uuidv4(),\n      title,\n      content,\n      ownerId: user.id,\n      published: true,\n    };\n\n    const savedArticle = await database.saveArticle(article);\n    currentSpan.setAttributes(savedArticle);\n    \n    const cacheResponse = await cache.set(savedArticle.id, savedArticle);\n    currentSpan.setAttributes(cacheResponse);\n    \n    const response = { message: \'Article created\', article };\n    return c.json(response, 201);\n  } catch (error) {\n    currentSpan.recordException(error);\n    return c.json({ error: \'Internal Error\' }, 500);\n  }\n});\n```\n\nWe removed the boilerplate from our code, the business logic is much more legible. OpenTelemetry is responsible for capturing timestamps, environment details, request headers, trace and span IDs, and facilitates sending traces to a vendor, or a self-hosted solution.\n\n## Misconceptions\n\n### Wide events replace metrics\n\nNo, they don\'t replace **all** metrics. Afaik, you cannot replace CPU metrics of your Kafka box with wide events. I\'m sure a very determined engineer can replace all metrics with wide events, but is it worth it? For monitoring infrastructure, metrics are your best option. They are insanely cheap and capture what you need to know about your infra.\n\nWhere they fail is complex application logic where unknown unknowns are most likely to occur. You definitely should replace all your application metrics with wide events.\n\n### Wide events are useful only during outages\n\nNo, the beauty of wide events is their context-richness. I have seen product teams use observability data for product analytics, simply because the tooling is more advanced. The issue preventing this from being more widespread is retention periods. Product analytics generally require year+ retention periods whereas observability less.\n\n### You must emit a single wide event per service\n\nEarlier I said you should emit a single wide event per service. I lied. There are no rules. Emit as many wide events as you need per request per service. Ideally only one, but there are scenarios where it\'s very valid to emit more than one wide event per service. But make sure to emit wide events, not events with 3 fields. And when emitting the new event, ask yourself if you\'re not repeating data in both events.\n\n### Logs, metrics and traces are the 3 pillars of observability\n\nThis is debunked. There are no pillars of observability. Observability is about answering the most uncommon questions about your app. Logs, metrics, traces, wide events, errors, etc. are just _data_. Data you should be able to query as you wish, asking questions about your app.\n\nYou don\'t see the 3 pillars of data analytics, why should there be 3 pillars of observability?\n\n### Structured logs are wide events\n\nFalse. Structured logs could be wide events, but not all structured logs are wide events. A structured log with 5 fields is not a wide event. A structured log with no context is not a wide event. If you print the response of a request without the details about the request (headers, path, method, body, etc.), it will be wide, but will it have the context you need to answer unknown unknown questions?\n\n### OpenTelemetry is the only modern way to do distributed tracing\n\nNo. I recommend distributed tracing, but it has a lot of flaws. It\'s grown to try to do too many things for too many people, and that\'s a problem. Instrumenting an application shouldn\'t be harder than building the application itself.\n\nSome vendors provide their own tracing, sometimes inspired by OpenTelemetry. Some teams have decided to add context propagation and timestamp capture to wide events and that\'s their tracing. It\'s up to you.\n\n---\n\nIn a nutshell, once you\'ve solved an issue with a wide events flow, you\'ll never want to go back to greping logs or metrics telling you "there\'s a problem", but no way to drill deeper to know the cause of the problem. With wide-events you stop investigating symptomps so you can focus on finding root causes.',
    },
    {
        "title": "Lessons from self-hosting ClickHouse",
        "link": "https://boristane.com/talks/lessons-from-self-hosting-clickhouse/",
        "guid": "https://boristane.com/talks/lessons-from-self-hosting-clickhouse/",
        "description": "We self-hosted ClickHouse for nearly 3 years, what did we learn?",
        "pubDate": "Tue, 18 Jun 2024 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "We self-hosted ClickHouse at [Baselime](https://baselime.io) for nearly 3 years. We broke things quite often and I shared with the London ClickHouse community a few of the lessons we learned in the process.",
    },
    {
        "title": "Lessons from starting, building, and exiting a devtools startup",
        "link": "https://boristane.com/blog/learnings-from-starting-building-and-exiting-a-devtools-startup/",
        "guid": "https://boristane.com/blog/learnings-from-starting-building-and-exiting-a-devtools-startup/",
        "description": "Key learnings from building Baselime",
        "pubDate": "Fri, 07 Jun 2024 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "I started [Baselime](https://baselime.io), an observability startup, in October 2021. 2.5 years later [we joined Cloudflare](https://assets/blog.cloudflare.com/cloudflare-acquires-baselime-expands-observability-capabilities). I was a first-time solo founder and I made more mistakes than I could count. \n\nIn this post, I'm reflecting on my time leading Baselime. I believe it will resonate with technical founders currently building or considering building similar companies. It focuses on early-stage, pre- Product Market Fit (PMF) developer tooling or infrastructure startups. I'll refrain from typical advice such as \"build a strong team\", or \"build something people want\", smarter people already [wrote about this](https://www.paulgraham.com/articles.html) extensively.\n\n## Be obsessed with finding Product Market Fit\n\nYour one and only mission as an early-stage founder is to [find Product Market Fit](https://www.sequoiacap.com/article/pmf-framework/) as quickly as possible. Nothing else matters.\n\nIt's extremely easy to focus on software architecture, clean code, tests, scalability, refactoring, etc.\n\nThese things don't matter.\n\n![test](/assets/blog/learnings-from-starting-building-and-exiting-a-devtools-startup/build-sell.jpg)\n\nI can hear you thinking that your startup is different and you need scalability from day 1. You're wrong. Your solution will evolve. Obviously, it's easier to evolve if the foundations are solid, but that's assuming the solid foundations you're building are for the right thing. Your early thesis about the world is most likely incorrect and you'll have to adjust.\n\nI spent too much time building the foundations for \"[Observability as Code](https://baselime.io/assets/blog/why-observability-as-code)\". Imagine the best parts of CloudFormation and Terraform for observability: a CLI and a state management backend. Developers write multiple YAML files (with variables, interpolation, templates, etc.), the CLI combines them into a giant JSON file, uploads it to the backend which decides which resources (alerts, queries, dashboards, etc.) to create, edit or delete and the order of those operations.\n\nDespite all the hours spent building, refining and improving this, nobody cared. I was more concerned about building an elegant solution than actually finding PMF. Don't do this. \n\nInstead, iterate fast, measure the result of your experiments, and if something doesn't work, throw it away and move on quickly. When you find what works, you'll refactor and improve.\n\n## Better, faster, cheaper is not enough anymore\n\nIt's common startup wisdom that your solution should be [better, faster, cheaper](https://youtu.be/opkHJLVAM4A) than the competition. Whilst there's truth in this statement, in 2024 this is not enough anymore. Incumbents in the developer tools space iterate fast, and there are 5 new startups going after your market every quarter.\n\nInstead, look for technological shifts and focus your efforts where you're a monopoly, rather than competing.\n\n![test](/assets/blog/learnings-from-starting-building-and-exiting-a-devtools-startup/monopoly.jpg)\n\nI started Baselime with a focus on observability for AWS Lambda. There was a consensus among our early users that [Baselime was better, faster and cheaper](https://x.com/patjakubik/status/1687204692915539970) than the competition. But this wasn't enough to stand out in a crowded space.\n\nUntil I noticed a massive segment where we could be a monopoly: the new developer platforms such as [Cloudflare](https://www.cloudflare.com/en-gb/developer-platform/), [Vercel](https://vercel.com), [Fly.io](https://fly.io), [Koyeb](https://www.koyeb.com/), [Render](https://render.com/), etc.\n\nWithin 1 week, we built a [Vercel integration](https://vercel.com/integrations/baselime) and instantly doubled our weekly active users. Likewise for our [Cloudflare integration](https://developers.cloudflare.com/workers/observability/baselime-integration/).\n\n## Don't be stubborn\n\nIf something is not growing fast, drop it quickly and work on something else. There are no points for stubbornness, you're in a race against running out of money before PMF.\n\nIn April 2023, I spoke with the Cloudflare team about observability on the Workers platform, but we built our Cloudflare integration only 6 months later. It actually took only 1 week to build it.\n\nWhy wait 6 months to build something so obvious? 6 months is an eternity for an early-stage startup.\n\nDuring this time, I pretended to \"[focus on a single problem](https://paulgraham.com/startupideas.html)\" to justify being stubborn on becoming the best observability platform for AWS Lambda. I ignored all market signals, both from developers and market research, telling me to shift my focus to an adjacent problem. Don't do this.\n\n## Do things that don't scale, really\n\nWe're developers, we want to automate everything. But sometimes you've got to roll up your sleeves and do the tedious, unsexy but necessary work to grow.\n\nWe built sales automation and lead generation pipelines, but nothing worked. Our user growth was flat. Until I decided to ignore all the best practices and do what works for me, something I'll actually enjoy: personally message every developer with a slight interest in cloud computing I could find on Twitter. It was unglamorous, tedious, and monotonous, but it had to be done. This directly translated to signups, sales and referrals.\n\nI should have done this months earlier.\n\n![test](/assets/blog/learnings-from-starting-building-and-exiting-a-devtools-startup/scale.png)\n\n## Create hype, don't be shy\n\n[If you build it, they won't come](https://www.forbes.com/sites/georgedeeb/2013/11/06/if-you-build-it-they-may-not-come-budget-ahead-for-startup-marketing/). You must create hype around your product. Technical founders tend to view hype as a bad thing, used to sell snake oil.\n\nWhen in fact, hype is a cheat code to get in the public awareness. Get developers to know your solution exists, its benefits and its limitations. It doesn't matter if they don't sign up today. Eventually (hopefully in the near future) they, or someone they know, will need to solve your problem and your solution should be the first they consider. Hype will get you there faster.\n\n<blockquote class=\"twitter-tweet\">\n<p lang=\"en\" dir=\"ltr\">you thought we were done?<br /><br />distributed tracing for <a href=\"https://twitter.com/CloudflareDev?ref_src=twsrc%5Etfw\">@CloudflareDev</a> with multi-cloud support <br /><br />- automatic correlation between logs and traces<br />- support for workers, kv, durable objects, queues<br/>- all powered by the <a href=\"https://twitter.com/baselimehq?ref_src=twsrc%5Etfw\">@baselimehq</a> query engine<br/><br/>and this design 😍✨<br/><br/>my dms are open if… <a href=\"https://t.co/KcLZSr4DQP\">pic.twitter.com/KcLZSr4DQP</a></p>&mdash; boris tane (@boristane) <a href=\"https://twitter.com/boristane/status/1717590624596083046?ref_src=twsrc%5Etfw\">October 26, 2023</a>\n</blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n## Get a side project\n\n[I love building software](/projects). It's my main hobby. But building Baselime meant I didn't have a hobby anymore. I didn't have space to do this thing I love without the pressure of solving a problem for developers.\n\nI kept shipping every day, but there are days I actually despised building software, I didn't want to build software but I had to.\n\nGet a side project, where you can explore and build whatever random useless thing you want. Your overall happiness levels will thank you - building a startup is hard enough, don't go through it without hobbies :)\n\n## Exiting a company is hard\n\nIf you thought starting a company is hard, try selling one.\n\nBaselime was a pretty simple business when we joined Cloudflare. Just a few employees and our main assets were our IP. However, it still took quite a bit of work, from the negotiations phase to all the accounting and legal work involved.\n\nThe most difficult part was how nerve wracking it all was. I was a solo-founder and until Baselime, the most complex contracts I ever signed were tenancy agreements.\n\nAfter 2 years of building and selling, Baselime was part of my identity. The reality of joining a larger organisation was challenging. Thankfully a few trusted advisors truly supported me throughout the process. Joining Cloudflare was the best decision to achieve our vision: enabling every developer to resolve issues before they become problems.\n\n---\n\nThe Baselime journey taught me much more than these highlights. I'm always keen to connect with dev tool founders and aspiring founders. Hit me up, [my dms are open](https://x.com/boristane) :)",
    },
    {
        "title": "Craft your own serverless observability platform",
        "link": "https://boristane.com/talks/craft-your-own-serverless-observability-platform/",
        "guid": "https://boristane.com/talks/craft-your-own-serverless-observability-platform/",
        "description": "How do you build your own observability platform? And should you?",
        "pubDate": "Wed, 12 Jul 2023 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": 'I had the opportunity to speak at the [Serverless London Meetup](https://www.meetup.com/serverless-london/) once again and this time I wanted to share with developers how they can build their own observability tooling using serverless primitives and host it themselves.\n\nIt was pretty fun to break down Baselime into its core components and share with developers what goes on in each, and how they can build it themselves.\n\nFun fact: for some reason I needed to use my mouse instead of a pointer to flick through slides - this was way more challenging than I anticipated.\n\n<script src="//fast.wistia.com/embed/medias/7o2q7h3rb0.jsonp" async></script> <script src="//fast.wistia.com/assets/external/E-v1.js" async></script> <div class="wistia_responsive_padding" style="padding:56.25% 0 0 0;position:relative;"> <div class="wistia_responsive_wrapper" style="height:100%;left:0;position:absolute;top:0;width:100%;"> <div class="wistia_embed wistia_async_7o2q7h3rb0 seo=false videoFoam=true" style="height:100%;width:100%">&nbsp;</div></div></div>',
    },
    {
        "title": "Observability with ClickHouse",
        "link": "https://boristane.com/talks/observability-with-clickhouse/",
        "guid": "https://boristane.com/talks/observability-with-clickhouse/",
        "description": 'A "behind the scenes" view into building an observability solution with ClickHouse.',
        "pubDate": "Wed, 24 May 2023 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "When I started [Baselime](https://baselime.io), one of the key shifts I noticed was the emergence of [ClickHouse](https://clickhouse.com) as a database for analytics, and I quickly realised it would be extremely good for high-cardinality and dimentionality data, which are two essential properties of data used for observability. That's why the Baselime data layer is powered by a self-hosted ClickHouse cluster.\n\nWhen the ClickHouse team invited me to speak at their London meetup, I was so excited to share what we'd learned building an observability platform on top of ClickHouse, for our ingestion pipelines to our database schemas, etc.",
    },
    {
        "title": "Creating an innovation engine with observability v2",
        "link": "https://boristane.com/talks/creating-an-innovation-engine-with-observability-v2/",
        "guid": "https://boristane.com/talks/creating-an-innovation-engine-with-observability-v2/",
        "description": "I gave this talk a second time, with corrections and improvements :)",
        "pubDate": "Thu, 13 Apr 2023 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "I was invited to give a talk at [Node Congress 2023](https://portal.gitnation.org/events/node-congress-2023); I spent quite a few days wondering what I could present; I had given a talk about [creating an innovation engine with observability](./creating-an-innovation-engine-with-observability-v1) before, which people liked and I knew it could be improved and resonate with even more developers.\n\nAnd that's exactly what I did. I made a few key changes to the talk and delivered it again. All about the culture changes that teams can make today to produce quality software without compromising on iteration speed.\n\nNode Congress doesn't allow embedding talk recordings, so you can find it on [their website](https://portal.gitnation.org/contents/creating-an-innovation-engine-with-observability).",
    },
    {
        "title": "Creating an innovation engine with observability v1",
        "link": "https://boristane.com/talks/creating-an-innovation-engine-with-observability-v1/",
        "guid": "https://boristane.com/talks/creating-an-innovation-engine-with-observability-v1/",
        "description": "How can you leverage your observability tooling to innovate faster?",
        "pubDate": "Wed, 01 Mar 2023 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "The [Serverless London Meetup](https://www.meetup.com/serverless-london/) invited me to give a talk, and I wanted to share something different: what's the engineering culture that enables us at [Baselime](https://baselime.io) to innovate extremely fast?\n\nOne of the key points is observability. How we leverage our observability tooling to have high velocity and quality software at the same time.\n\nI gave this talk twice, and this is was the first iteration. In the second iteration, at the [Node Congress 2023 conference](./creating-an-innovation-engine-with-observability-v2), I incorporated feedback and improvements from this talk.",
    },
    {
        "title": "Exploring JavaScript Runtimes on AWS Lambda",
        "link": "https://boristane.com/talks/javascript-runtimes-on-aws-lambda/",
        "guid": "https://boristane.com/talks/javascript-runtimes-on-aws-lambda/",
        "description": "Benchmarking a few JavaScript runtimes on AWS Lambda",
        "pubDate": "Mon, 21 Nov 2022 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "[London Node User Group](https://lnug.org/) invited me to give a talk, and that came up when I was wondering if I could squeeze more performance out of my JavaScript [AWS Lambda](https://aws.amazon.com/lambda/) functions without re-writing in Go or Rust.\n\nI then benchmarked [Node.js](https://nodejs.org/en), [Deno](https://deno.com/) and [Bun](https://bun.sh/) on AWS Lambda... The benchmark results themselves are not quite relevant as those runtimes are evolving very quickly; the most interesting part was the journey of deploying unsupported runtimes on AWS Lambda.\n\nThis talk is about how to deploy custom runtimes on AWS Lambda, and measure and compare the duration of HTTP requests made to those functions. It's also a great demo of a very early version of [Baselime](https://baselime.io).",
    },
    {
        "title": "Serverless Transactional Outbox Pattern on AWS",
        "link": "https://boristane.com/talks/serverless-transactional-outbox-pattern-on-aws/",
        "guid": "https://boristane.com/talks/serverless-transactional-outbox-pattern-on-aws/",
        "description": "How I implemented a transactional outbox pattern on AWS with serverless services",
        "pubDate": "Mon, 16 May 2022 00:00:00 GMT",
        "author": "Boris Tane",
        "content_encoded": "I believe this is my first ever public tech talk. The folks at [SST](https://sst.dev/) did the [SST Conf v1](https://v1conf.sst.dev/) for the launch of SST v1 and were kind enough to invite me. It was all remote and it was for me the occasion to connect with a few developers in the serverless community.\n\nMy talk was about how I built a [transactional outbox pattern](https://microservices.io/patterns/data/transactional-outbox.html) on AWS with serverless services for [Baselime](https://baselime.io). The main idea of this pattern is to resolve a problem with event driven architectures: how do you atomically update the database and send events to your message broker?\n\nMy talk starts at [29:53](https://youtu.be/6FzLjpMYcu8?t=1793).",
    },
]
